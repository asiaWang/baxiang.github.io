<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Audio | 码农人生]]></title>
  <link href="http://msching.github.io/blog/categories/audio/atom.xml" rel="self"/>
  <link href="http://msching.github.io/"/>
  <updated>2015-01-29T09:48:07+08:00</updated>
  <id>http://msching.github.io/</id>
  <author>
    <name><![CDATA[ChengYinZju]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (八)：NowPlayingCenter和RemoteControl]]></title>
    <link href="http://msching.github.io/blog/2014/11/06/audio-in-ios-8/"/>
    <updated>2014-11-06T13:26:28+08:00</updated>
    <id>http://msching.github.io/blog/2014/11/06/audio-in-ios-8</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>距离<a href="/blog/2014/09/07/audio-in-ios-7/">上一篇</a>博文发布已经有一个月多的时间了，在这其间我一直忙于筹办婚礼以至于这篇博文一直拖到了现在。</p>

<p>在之前<a href="/blog/categories/ios-audio/">一到六篇</a>中我对iOS下的音频播放流程进行了阐述，在<a href="/blog/2014/09/07/audio-in-ios-7/">第七篇</a>中介绍了如何播放iPod Lib中的歌曲，至此有关音频播放的话题就已经完结了，在这篇里我将会讲到的<code>NowPlayingCenter</code>和<code>RemoteControl</code>这两个玩意本身和整个播放流程并没有什么关系，但它们可以让音频播放在iOS系统上获得更加好的用户体验。</p>

<hr />

<h1>NowPlayingCenter</h1>

<p><code>NowPlayingCenter</code>能够显示当前正在播放的歌曲信息，它可以控制的范围包括：</p>

<ul>
<li>锁频界面上所显示的歌曲播放信息和图片</li>
<li>iOS7之后控制中心上显示的歌曲播放信息</li>
<li>iOS7之前双击home键后出现的进程中向左滑动出现的歌曲播放信息</li>
<li>AppleTV，AirPlay中显示的播放信息</li>
<li>车载系统中显示的播放信息</li>
</ul>


<p>这些信息的显示都由<code>MPNowPlayingInfoCenter</code>类来控制，这个类的定义非常简单：</p>

<p>```objc
MP_EXTERN_CLASS_AVAILABLE(5_0) @interface MPNowPlayingInfoCenter : NSObject</p>

<p>// Returns the default now playing info center.
// The default center holds now playing info about the current application.
+ (MPNowPlayingInfoCenter *)defaultCenter;</p>

<p>// The current now playing info for the center.
// Setting the info to nil will clear it.
@property (copy) NSDictionary *nowPlayingInfo;</p>

<p>@end
```</p>

<p>使用也同样简单，首先<code>#import &lt;MediaPlayer/MPNowPlayingInfoCenter.h&gt;</code>然后调用<code>MPNowPlayingInfoCenter</code>的单例方法获取实例，再把需要显示的信息组织成Dictionary并赋值给<code>nowPlayingInfo</code>属性就完成了。</p>

<p><code>nowPlayingInfo</code>中一些常用属性被定义在<code>&lt;MediaPlayer/MPMediaItem.h&gt;</code>中</p>

<p><code>objc
MPMediaItemPropertyAlbumTitle                //NSString
MPMediaItemPropertyAlbumTrackCount          //NSNumber of NSUInteger
MPMediaItemPropertyAlbumTrackNumber     //NSNumber of NSUInteger
MPMediaItemPropertyArtist                   //NSString
MPMediaItemPropertyArtwork                  //MPMediaItemArtwork
MPMediaItemPropertyComposer             //NSString
MPMediaItemPropertyDiscCount                //NSNumber of NSUInteger
MPMediaItemPropertyDiscNumber               //NSNumber of NSUInteger
MPMediaItemPropertyGenre                    //NSString
MPMediaItemPropertyPersistentID         //NSNumber of uint64_t
MPMediaItemPropertyPlaybackDuration     //NSNumber of NSTimeInterval
MPMediaItemPropertyTitle                    //NSString
</code></p>

<p>上面这些属性大多比较浅显易懂，基本上按照字面上的意思去理解就可以了，需要稍微解释以下的是<code>MPMediaItemPropertyArtwork</code>。这个属性表示的是锁屏界面或者AirPlay中显示的歌曲封面图，<code>MPMediaItemArtwork</code>类可以由<code>UIImage</code>类进行初始化。</p>

<p>```objc
MP_EXTERN_CLASS_AVAILABLE(3_0) @interface MPMediaItemArtwork : NSObject</p>

<p>// Initializes an MPMediaItemArtwork instance with the given full-size image.
// The crop rect of the image is assumed to be equal to the bounds of the
// image as defined by the image&rsquo;s size in points, i.e. tightly cropped.
&ndash; (instancetype)initWithImage:(UIImage *)image NS_DESIGNATED_INITIALIZER NS_AVAILABLE_IOS(5_0);</p>

<p>// Returns the artwork image for an item at a given size (in points).
&ndash; (UIImage *)imageWithSize:(CGSize)size;</p>

<p>@property (nonatomic, readonly) CGRect bounds; // The bounds of the full size image (in points).
@property (nonatomic, readonly) CGRect imageCropRect; // The actual content area of the artwork, in the bounds of the full size image (in points).</p>

<p>@end
```</p>

<p>另外一些附加属性被定义在<code>&lt;MediaPlayer/MPNowPlayingInfoCenter.h&gt;</code>中</p>

<p>```objc
// The elapsed time of the now playing item, in seconds.
// Note the elapsed time will be automatically extrapolated from the previously
// provided elapsed time and playback rate, so updating this property frequently
// is not required (or recommended.)
MP_EXTERN NSString *const MPNowPlayingInfoPropertyElapsedPlaybackTime NS_AVAILABLE_IOS(5_0); // NSNumber (double)</p>

<p>// The playback rate of the now playing item, with 1.0 representing normal
// playback. For example, 2.0 would represent playback at twice the normal rate.
// If not specified, assumed to be 1.0.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyPlaybackRate NS_AVAILABLE_IOS(5_0); // NSNumber (double)</p>

<p>// The &ldquo;default&rdquo; playback rate of the now playing item. You should set this
// property if your app is playing a media item at a rate other than 1.0 in a
// default playback state. e.g., if you are playing back content at a rate of
// 2.0 and your playback state is not fast-forwarding, then the default
// playback rate should also be 2.0. Conversely, if you are playing back content
// at a normal rate (1.0) but the user is fast-forwarding your content at a rate
// greater than 1.0, then the default playback rate should be set to 1.0.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyDefaultPlaybackRate NS_AVAILABLE_IOS(8_0); // NSNumber (double)</p>

<p>// The index of the now playing item in the application&rsquo;s playback queue.
// Note that the queue uses zero-based indexing, so the index of the first item
// would be 0 if the item should be displayed as &ldquo;item 1 of 10&rdquo;.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyPlaybackQueueIndex NS_AVAILABLE_IOS(5_0); // NSNumber (NSUInteger)</p>

<p>// The total number of items in the application&rsquo;s playback queue.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyPlaybackQueueCount NS_AVAILABLE_IOS(5_0); // NSNumber (NSUInteger)</p>

<p>// The chapter currently being played. Note that this is zero-based.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyChapterNumber NS_AVAILABLE_IOS(5_0); // NSNumber (NSUInteger)</p>

<p>// The total number of chapters in the now playing item.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyChapterCount NS_AVAILABLE_IOS(5_0); // NSNumber (NSUInteger)
<code>``
其中常用的是</code>MPNowPlayingInfoPropertyElapsedPlaybackTime<code>和</code>MPNowPlayingInfoPropertyPlaybackRate`：</p>

<ul>
<li><code>MPNowPlayingInfoPropertyElapsedPlaybackTime</code>表示已经播放的时间，用这个属性可以让<code>NowPlayingCenter</code>显示播放进度；</li>
<li><code>MPNowPlayingInfoPropertyPlaybackRate</code>表示播放速率。通常情况下播放速率为1.0，即真是时间的1秒对应播放时间中的1秒；</li>
</ul>


<p>这里需要解释的是，<code>NowPlayingCenter</code>中的进度刷新并不是由app不停的更新<code>nowPlayingInfo</code>来做的，而是根据app传入的<code>ElapsedPlaybackTime</code>和<code>PlaybackRate</code>进行自动刷新。例如传入ElapsedPlaybackTime=120s，PlaybackRate=1.0，那么<code>NowPlayingCenter</code>会显示2:00并且在接下来的时间中每一秒把进度加1秒并刷新显示。如果需要暂停进度，传入PlaybackRate=0.0即可。</p>

<p>所以每次播放暂停和继续都需要更新<code>NowPlayingCenter</code>并正确设置<code>ElapsedPlaybackTime</code>和<code>PlaybackRate</code>否则<code>NowPlayingCenter</code>中的播放进度无法正常显示。</p>

<h3>NowPlayingCenter的刷新时机</h3>

<p>频繁的刷新<code>NowPlayingCenter</code>并不可取，特别是在有Artwork的情况下。所以需要在合适的时候进行刷新。</p>

<p>依照我自己的经验下面几个情况下刷新<code>NowPlayingCenter</code>比较合适：</p>

<ul>
<li>当前播放歌曲进度被拖动时</li>
<li>当前播放的歌曲变化时</li>
<li>播放暂停或者恢复时</li>
<li>当前播放歌曲的信息发生变化时（例如Artwork，duration等）</li>
</ul>


<p>在刷新时可以适当的通过判断app是否active来决定是否必须刷新以减少刷新次数。</p>

<h3>MPMediaItemPropertyArtwork</h3>

<p>这是一个非常有用的属性，我们可以利用歌曲的封面图来合成一些图片借此达到美化锁屏界面或者显示锁屏歌词。</p>

<hr />

<h1>RemoteControl</h1>

<p><code>RemoteComtrol</code>可以用来在不打开app的情况下控制app中的多媒体播放行为，涉及的内容主要包括：</p>

<ul>
<li>锁屏界面双击Home键后出现的播放操作区域</li>
<li>iOS7之后控制中心的播放操作区域</li>
<li>iOS7之前双击home键后出现的进程中向左滑动出现的播放操作区域</li>
<li>AppleTV，AirPlay中显示的播放操作区域</li>
<li>耳机线控</li>
<li>车载系统的设置</li>
</ul>


<h3>在何处处理RemoteComtrol</h3>

<p>根据<a href="https://developer.apple.com/library/ios/documentation/EventHandling/Conceptual/EventHandlingiPhoneOS/Remote-ControlEvents/Remote-ControlEvents.html">官方文档</a>的描述：</p>

<p><code>If your app plays audio or video content, you might want it to respond to remote control events that originate from either transport controls or external accessories. (External accessories must conform to Apple-provided specifications.) iOS converts commands into UIEvent objects and delivers the events to an app. The app sends them to the first responder and, if the first responder doesn’t handle them, they travel up the responder chain.</code></p>

<p>当<code>RemoteComtrol</code>事件产生时，iOS会以<code>UIEvent</code>的形式发送给app，app会首先转发到first responder，如果first responder不处理这个事件的话那么事件就会沿着responder chain继续转发。关于responder chain的相关内容可以查看<a href="https://developer.apple.com/library/IOs/documentation/General/Conceptual/Devpedia-CocoaApp/Responder.html">这里</a>。</p>

<p>从responder chain文档看来如果之前的所有responder全部不响应<code>RemoteComtrol</code>事件的话，最终事件会被转发给Application（如图）。所以我们知道作为responder chain的最末端，在<code>UIApplication</code>中实现<code>RemoteComtrol</code>的处理是最为合理的，而并非在UIWindow中或者AppDelegate中。</p>

<p><img src="/images/iOS-audio/responder%20chain.jpg" alt="" /></p>

<h3>实现自己的UIApplication</h3>

<p>首先新建一个<code>UIApplication</code>的子类</p>

<p>```objc</p>

<h1>import &lt;UIKit/UIKit.h></h1>

<p>@interface MyApplication : UIApplication</p>

<p>@end
```</p>

<p>然后找到工程中的<code>main.m</code>，可以看到代码如下：</p>

<p>```c
int main(int argc, char * argv[])
{</p>

<pre><code>@autoreleasepool {
    return UIApplicationMain(argc, argv, nil, NSStringFromClass([AppDelegate class]));
}
</code></pre>

<p>}
<code>``
在main中调用了</code>UIApplicationMain`方法</p>

<p><code>objc
// If nil is specified for principalClassName, the value for NSPrincipalClass from the Info.plist is used. If there is no
// NSPrincipalClass key specified, the UIApplication class is used. The delegate class will be instantiated using init.
UIKIT_EXTERN int UIApplicationMain(int argc, char *argv[], NSString *principalClassName, NSString *delegateClassName);
</code></p>

<p>我们需要做的就是给<code>UIApplicationMain</code>方法的第三个参数传入我们的application类名，如下：</p>

<p>```c</p>

<h1>import &lt;UIKit/UIKit.h></h1>

<h1>import &ldquo;AppDelegate.h&rdquo;</h1>

<h1>import &ldquo;MyApplication.h&rdquo;</h1>

<p>int main(int argc, char * argv[])
{</p>

<pre><code>@autoreleasepool {
    return UIApplicationMain(argc, argv, NSStringFromClass([MyApplication class]), NSStringFromClass([AppDelegate class]));
}
</code></pre>

<p>}
```</p>

<p>这样就成功实现了自己的<code>UIApplication</code>.</p>

<h3>处理RemoteComtrol</h3>

<p>了解了应该在何处处理<code>RemoteComtrol</code>事件之后，再来看下<a href="https://developer.apple.com/library/ios/documentation/EventHandling/Conceptual/EventHandlingiPhoneOS/Remote-ControlEvents/Remote-ControlEvents.html">官方文档</a>中描述的三个必要条件：</p>

<ul>
<li>接受者必须能够成为first responder</li>
<li>必须显示地声明接收<code>RemoteComtrol</code>事件</li>
<li>你的app必须是<code>Now Playing</code>app</li>
</ul>


<p>对于第一条就是要在自己的<code>UIApplication</code>中实现<code>canBecomeFirstResponder</code>方法:</p>

<p>```objc</p>

<h1>import &ldquo;MyApplication.h&rdquo;</h1>

<p>@implementation MyApplication</p>

<ul>
<li>(BOOL)canBecomeFirstResponder
{
  return YES;
}</li>
</ul>


<p>@end
<code>``
第二条是要求显示地调用</code>[[UIApplication sharedApplication] beginReceivingRemoteControlEvents]`，调用的实际一般是在播放开始时；</p>

<p>第三条就是要求占据NowPlayingCenter，这个之前已经提到过了。</p>

<p>满足三个条件后可以在<code>UIApplication</code>中实现处理<code>RemoteComtrol</code>事件的方法，根据不同的事件实现不同的操作即可。</p>

<p>```objc</p>

<h1>import &ldquo;MyApplication.h&rdquo;</h1>

<p>@implementation MyApplication</p>

<ul>
<li><p>(BOOL)canBecomeFirstResponder
{
  return YES;
}</p></li>
<li><p>(void)remoteControlReceivedWithEvent:(UIEvent *)event
{
  switch (event.subtype)
  {
      case UIEventSubtypeRemoteControlPlay:
            //play
          break;
      case UIEventSubtypeRemoteControlPause:
            //pause
          break;
      case UIEventSubtypeRemoteControlStop:
            //stop
          break;
      default:
          break;
  }
}</p></li>
</ul>


<p>@end
```</p>

<hr />

<h1>示例代码</h1>

<p>git上有一个关于remotecontrol的小工程供大家参考<a href="https://github.com/MosheBerman/ios-audio-remote-control">ios-audio-remote-control</a></p>

<hr />

<h1>后记</h1>

<p>到本篇为止iOS的音频播放话题基本上算是完结了。接下来我会在空余时间去研究一下iOS 8中新加入的<code>AVAudioEngine</code>，其功能涵盖播放、录音、混音、音效处理，看上去十分强大，从接口的定义上看像是对<code>AudioUnit</code>的高层封装，当研究有了一定的成果之后也会以博文的形式分享出来。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://encrypted.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CC4QFjAA&amp;url=https%3A%2F%2Fdeveloper.apple.com%2FLibrary%2Fios%2Fdocumentation%2FMediaPlayer%2FReference%2FMPNowPlayingInfoCenter_Class%2Findex.html&amp;ei=8bBhVO_RF4HKmwXBiILIDA&amp;usg=AFQjCNFOziF2zKft-wGQ3ew_cHy7Ivxrvg">MPNowPlayingInfoCenter</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/EventHandling/Conceptual/EventHandlingiPhoneOS/Remote-ControlEvents/Remote-ControlEvents.html">Remote Control Events</a></p>

<p><a href="https://developer.apple.com/library/IOs/documentation/General/Conceptual/Devpedia-CocoaApp/Responder.html">Cocoa Responder Chain</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (七)：播放iPod Library中的歌曲]]></title>
    <link href="http://msching.github.io/blog/2014/09/07/audio-in-ios-7/"/>
    <updated>2014-09-07T15:45:47+08:00</updated>
    <id>http://msching.github.io/blog/2014/09/07/audio-in-ios-7</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>由于最近工作量非常饱和，所以这第七篇来的有点晚（创建时间是9月7日。。说出来都是泪）。</p>

<p>现在市面上的音乐播放器都支持iPod Library歌曲（俗称iPod音乐或者本地音乐）的播放，用户对于iPod音乐播放的需求也一直十分强烈。这篇要讲的是如何来播放iPod Library的歌曲。</p>

<hr />

<h1>概述</h1>

<p>根据<a href="https://developer.apple.com/library/ios/documentation/audiovideo/conceptual/multimediapg/usingaudio/usingaudio.html#//apple_ref/doc/uid/TP40009767-CH2-SW43">官方文档</a>描述Apple从iOS 3.0开始允许开发者访问用户的iPod library来获取用户放在其中的歌曲等多媒体内容。</p>

<p>为此Apple提供了多种方法来访问和播放iPod中的音乐，下面我们来分别列举一下这些方法。</p>

<hr />

<h1>访问MediaLibrary</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/AboutiPodLibraryAccess/AboutiPodLibraryAccess.html#//apple_ref/doc/uid/TP40008765-CH103-SW9">官方文档</a>访问iPod Library的方法有两种，分别是MediaPicker和MediaQuery。</p>

<p><img src="/images/iOS-audio/iPodLibraryAccessOverview.jpg" alt="" /></p>

<h3>MediaPicker</h3>

<p>MediaPicker是一个高度封装的iPod Library访问方式，通过使用<code>MPMediaPickerController</code>类来访问iPod Library。这是一个UI控件，用户可以根据需要选择其中的音乐。这个类使用时非常方便，只需要生成一个``的实例，设置一下属性和delegate后present出来，接下来只要等待回调即可，在回调时需要手动dismiss picker。</p>

<p>```objc
MPMediaPickerController *picker = [[MPMediaPickerController alloc] initWithMediaTypes:MPMediaTypeAnyAudio];
picker.prompt = @&ldquo;请选择需要播放的歌曲&rdquo;;
picker.showsCloudItems = NO;
picker.allowsPickingMultipleItems = YES;
picker.delegate = self;
[self presentViewController:picker animated:YES completion:nil];</p>

<ul>
<li><p>(void)mediaPickerDidCancel:(MPMediaPickerController *)mediaPicker
{
  [mediaPicker dismissViewControllerAnimated:YES completion:nil];
}</p></li>
<li><p>(void)mediaPicker:(MPMediaPickerController <em>)mediaPicker didPickMediaItems:(MPMediaItemCollection </em>)mediaItemCollection
{
  [mediaPicker dismissViewControllerAnimated:YES completion:nil];
  //do something
}
```</p></li>
</ul>


<p>上面的代码将会得到如下的效果：</p>

<p><img src="/images/iOS-audio/MediaPicker.jpg" alt="" /></p>

<p>通过MediaPicker最终可以得到<code>MPMediaItemCollection</code>，其中存放着所有在Picker中选中的歌曲，每一个歌曲使用一个<code>MPMediaItem</code>对象表示。对于MediaPicker的使用也可以参考<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingtheMediaItemPicker/UsingtheMediaItemPicker.html#//apple_ref/doc/uid/TP40008765-CH104-SW1">官方文档</a>。</p>

<h3>MediaQuery</h3>

<p>如果你觉得MeidaPicker的功能或者UI不能满足你的要求那么可以使用MediaQuery。MediaQuery可以直接访问iPod Library的DB，并根据需要获取数据。<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingTheiPodLibrary/UsingTheiPodLibrary.html#//apple_ref/doc/uid/TP40008765-CH101-SW1">官方文档</a>给出了MediaQuery的示意图。</p>

<p><img src="/images/iOS-audio/database_access_classes.jpg" alt="" /></p>

<p>MediaQuery功能十分强大，它可以根据一个或多个条件查询满足需要的MediaItem。</p>

<p>你可以使用<code>MPMediaQuery</code>的类方法来生成一些已经预置了条件的Query</p>

<p><code>objc
// Base queries which can be used directly or as the basis for custom queries.
// The groupingType for these queries is preset to the appropriate type for the query.
+ (MPMediaQuery *)albumsQuery;
+ (MPMediaQuery *)artistsQuery;
+ (MPMediaQuery *)songsQuery;
+ (MPMediaQuery *)playlistsQuery;
+ (MPMediaQuery *)podcastsQuery;
+ (MPMediaQuery *)audiobooksQuery;
+ (MPMediaQuery *)compilationsQuery;
+ (MPMediaQuery *)composersQuery;
+ (MPMediaQuery *)genresQuery;
</code>
也可以自己生成<code>MPMediaPredicate</code>设置条件，并把它加到Query中，最后通过items和collections访问查询到的结果，例如：</p>

<p>```objc
MPMediaPropertyPredicate *artistNamePredicate =
[MPMediaPropertyPredicate predicateWithValue:@&ldquo;Happy the Clown&rdquo;</p>

<pre><code>                             forProperty:MPMediaItemPropertyArtist
                          comparisonType:MPMediaPredicateComparisonEqualTo];
</code></pre>

<p>MPMediaQuery *quert = [[MPMediaQuery alloc] init];
[quert addFilterPredicate: artistNamePredicate];
quert.groupingType = MPMediaGroupingArtist;</p>

<p>NSArray <em>itemsFromArtistQuery = [quert items];
NSArray </em>collectionsFromArtistQuery = [quert collections];
```
这一过程可以表示为（图来自<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/AboutiPodLibraryAccess/AboutiPodLibraryAccess.html#//apple_ref/doc/uid/TP40008765-CH103-SW9">官方文档</a>）：</p>

<p><img src="/images/iOS-audio/mediaQuery.jpg" alt="" /></p>

<p>这里对于MediaQuery的用法就不再继续展开，关于这块内容并没有什么晦涩难懂的地方需要解释，大家可以通过阅读<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingTheiPodLibrary/UsingTheiPodLibrary.html#//apple_ref/doc/uid/TP40008765-CH101-SW1">官方文档</a>来详细了解其用法。</p>

<h3>MediaCollection</h3>

<p><code>MPMediaCollection</code>是MediaItem的合集，可以通过访问它的items属性来访问所有的MediaItem。</p>

<p><code>MPMediaPlaylist</code>是一个特殊的<code>MPMediaCollection</code>代表用户创建的播放列表，它会比MediaCollection包含更多的信息，比如播放列表的名字等等。这些属性可以通过<code>MPMediaEntity</code>的方法访问（MPMediaCollection是MPMediaEntity的子类，MPMediaItem也是）。
```objc
// Returns the value for the given entity property.
// MPMediaItem and MPMediaPlaylist have their own properties
&ndash; (id)valueForProperty:(NSString *)property;</p>

<p>// Executes a provided block with the fetched values for the given item properties, or nil if no value is available for a property.
// In some cases, enumerating the values for multiple properties can be more efficient than fetching each individual property with -valueForProperty:.
&ndash; (void)enumerateValuesForProperties:(NSSet <em>)properties usingBlock:(void (^)(NSString </em>property, id value, BOOL *stop))block NS_AVAILABLE_IOS(4_0);
```</p>

<h3>MediaItem</h3>

<p>通过MediaPicker和MediaQuery最终都会得到<code>MPMediaItem</code>，这个item中包含了许多信息。这些信息都可以通过<code>MPMediaEntity</code>的方法访问，其中参数非常多就不列举了具体可以参照MPMediaItem.h。</p>

<hr />

<h1>使用MPMusicPlayerController</h1>

<p>拿到iPod Library中的歌曲后就可以开始播放了。播放的方式有很多种，先介绍一下<code>MediaPlayer framework</code>中的<code>MPMusicPlayerController</code>类。</p>

<p>通过<code>MPMusicPlayerController</code>的类方法可以生成两种播放器，生成方法如下：</p>

<p>```objc
// Playing media items with the applicationMusicPlayer will restore the user&rsquo;s iPod state after the application quits.
+ (MPMusicPlayerController *)applicationMusicPlayer;</p>

<p>// Playing media items with the iPodMusicPlayer will replace the user&rsquo;s current iPod state.
+ (MPMusicPlayerController *)iPodMusicPlayer;
```</p>

<p>这两个方法看似生成了一样的对象，但它们的行为却有很大不同。从Apple写的注释上我们可以很清楚的发现它们的区别。<code>+applicationMusicPlayer</code>不会继承来自iOS系统自带的iPod应用中的播放状态，同时也不会覆盖iPod的播放状态。而<code>+iPodMusicPlayer</code>完全继承iPod应用的播放状态（甚至是播放时间），对其实例的任何操作也会覆盖到iPod应用。对<code>+iPodMusicPlayer</code>方法command+点击后可以看到更详细的注释。</p>

<p>```</p>

<pre><code>The iPod music player employs the iPod app on your behalf. On instantiation, it takes on the current iPod app state and controls that state as your app runs. Specifically, the shared state includes the following:
Repeat mode (see “Repeat Modes”)
Shuffle mode (see “Shuffle Modes”
Now-playing item (see nowPlayingItem)
Playback state (see playbackState)

Other aspects of iPod state, such as the on-the-go playlist, are not shared. Music that is playing continues to play when your app moves to the background.
</code></pre>

<p>```
说白了，当在使用iPodMusicPlayerv其实并不是你的程序在播放音频，而是你的程序在操纵iPod应用播放音频，即使你的程序crash了或者被kill了，音乐也不会因此停止。</p>

<p>而对于<code>+applicationMusicPlayer</code>通过command+点击可以看到：</p>

<p>```
The application music player plays music locally within your app. It does not affect the iPod state.
When your app moves to the background, the music player stops if it was playing.</p>

<p>```
从注释中可以知道这个方法返回的对象虽然不是调用iPod应用播放的也不会影响到iPod应用，但它有个很大的缺点：无法后台播放，即使你在active了audioSession并且在app的设置中设置了Background Audio同样不会奏效。</p>

<p>综上所述，一般在开发音乐软件时很少用到这两个接口来进行iPod Library的播放，大部分开发者都是用这个类中的volme来调整系统音量的（这个属性在SDK 7中也被deprecate掉了）。如果你想用到这个类进行播放的话，这里需要提个醒，给<code>MPMusicPlayerController</code>设置需要播放的音乐时要使用下面两个方法：</p>

<p><code>objc
// Call -play to begin playback after setting an item queue source. Setting a query will implicitly use MPMediaGroupingTitle.
- (void)setQueueWithQuery:(MPMediaQuery *)query;
- (void)setQueueWithItemCollection:(MPMediaItemCollection *)itemCollection;
</code>
而不是这个属性：</p>

<p><code>objc
// Returns the currently playing media item, or nil if none is playing.
// Setting the nowPlayingItem to an item in the current queue will begin playback at that item.
@property(nonatomic, copy) MPMediaItem *nowPlayingItem;
</code>
光看名字很容易被<code>nowPlayingItem</code>这个属性迷惑，它的意思其实是说在设置了MediaQuery或者MediaCollection之后再设置这个nowPlayingItem可以让播放器从这个item开始播放，前提是这个item需要在MediaQuery或者MediaCollection的.items集合内。</p>

<hr />

<h1>使用AVAudioPlayer和AVPlayer</h1>

<p>除了使用MediaPlayer中的类还有很多其他方法来进行iPod播放，其中做的比较出色的是<code>AVFoundation</code>中的<code>AVAudioPlayer</code>和<code>AVPlayer</code>。</p>

<p>这两个类的都有通过NSURL生成实例的初始化方法：</p>

<p>```objc
//AVAudioPlayer
&ndash; (id)initWithContentsOfURL:(NSURL *)url error:(NSError **)outError;</p>

<p>//AVPlayer
+ (id)playerWithURL:(NSURL <em>)URL;
&ndash; (id)initWithURL:(NSURL </em>)URL;
```</p>

<p>其中的NSURL正是来自于<code>MPMediaItem</code>的<code>MPMediaItemPropertyAssetURL</code>属性。</p>

<p><code>objc
//A URL pointing to the media item,
//from which an AVAsset object (or other URL-based AV Foundation object) can be created, with any options as desired.
//Value is an NSURL object.
MP_EXTERN NSString *const MPMediaItemPropertyAssetURL;
</code>
上面讲到<code>MPMediaItem</code>时已经提到了它是<code>MPMediaEntity</code>子类，可以通过<code>-valueForProperty:</code>方法访问其中的属性。通过传入<code>MPMediaItemPropertyAssetURL</code>就可以得到当前MediaItem对应的URL（ipod-library://xxxxx），生成Player进行播放。大致代码如下：</p>

<p>```objc
@interface MyClass : NSObject
{</p>

<pre><code>AVAudioPlayer *_player;
//AVPlayer *_player;
</code></pre>

<p>}</p>

<p>//设置AudioSession
[[AVAudioSession sharedInstance] setActive:YES error:nil];
[[AVAudioSession sharedInstance] setCategory:AVAudioSessionCategoryPlayback error:nil];</p>

<p>//play
NSError <em>error = nil;
MPMediaItem </em>item = &hellip;;
NSURL *url = [item valueForProperty:MPMediaItemPropertyAssetURL];
<em>player = [[AVAudioPlayer alloc] initWithContentsOfURL:url error:&amp;error];
//</em>player = [AVPlayer playerWithURL:url];
[_player play];</p>

<p>```</p>

<p><strong>注意：这里我需要更正一下，之前我在<a href="/blog/2014/07/08/audio-in-ios-2/">第二篇</a>讲到AudioSession时写了这样一段话<code>在使用AVAudioPlayer/AVPlayer时可以不用关心AudioSession的相关问题，Apple已经把AudioSession的处理过程封装了...</code>。这段话不对，我把AVFoundation和Mediaplayer混淆了，在写的时候也没注意，应该是在使用MPMusicPlayerController播放时不需要关心AudioSession的问题。</strong></p>

<hr />

<h1>读取和导出数据</h1>

<p>前面说到使用<code>MPMediaItem</code>的<code>MPMediaItemPropertyAssetURL</code>属性可以得到一个表示当前MediaItem的NSURL，有了这个NSURL我们使用AVFoundation中的类进行播放。播放只是最基本的需求，有了这个URL我们可以做更多更有趣的事情。</p>

<p>在AVFoundation中还有两个有趣的类：<code>AVAssetReader</code>和<code>AVAssetExportSession</code>。它们可以把iPod Library中的指定歌曲以指定的音频格式导出到内存中或者硬盘中，这个指定的格式包括PCM。这是一个激动人心的特性，有了PCM数据我们就可以做很多很多其他的事情了。</p>

<p>这部分如果要展开的话还会有相当多的内容，国外的先辈们早在2010年就已经发掘了这两个类的用法，详细参见<a href="http://www.subfurther.com/blog/2010/07/19/from-iphone-media-library-to-pcm-samples-in-dozens-of-confounding-potentially-lossy-steps/">这里</a>和<a href="http://www.subfurther.com/blog/2010/12/13/from-ipod-library-to-pcm-samples-in-far-fewer-steps-than-were-previously-necessary/">这里</a>。这两篇讲的比较详细并且附有Sample（其中还涉及了一些Extended Audio File Services的内容），如果里面Sample无法下载可以从点击<a href="/files/MediaLibraryExportThrowaway1.zip">MediaLibraryExportThrowaway1.zip</a>和<a href="/files/VTM_AViPodReader.zip">VTM_AViPodReader.zip</a>下载。</p>

<p><strong>需要注意的是在使用<code>AVAssetReader</code>的过程中如果访问系统的相机或者照片可能会使<code>AVAssetReader</code>产生<code>AVErrorOperationInterrupted</code>错误，此时需要重新生成Reader后调用<code>-startReading</code>才可以继续读取数据。</strong></p>

<hr />

<h1>小结</h1>

<p>本篇介绍了一些与iPod Library相关的内容，小结一下：</p>

<ul>
<li><p>Apple提供两种方法来访问iPod Library，它们分别是<code>MPMediaPickerController</code>和<code>MPMediaQuery</code>；</p></li>
<li><p><code>MPMediaPickerController</code>和<code>MPMediaQuery</code>最后输出给开发者的对象是<code>MPMediaItem</code>，<code>MPMediaItem</code>的属性需要通过<code>-valueForProperty:</code>方法获取了；</p></li>
<li><p><code>MPMusicPlayerController</code>可以用来播放<code>MPMediaItem</code>，但有很多局限性，使用时需要根据不同的使用场景来决定用哪个类方法生成实例；</p></li>
<li><p><code>AVAudioPlayer</code>和<code>AVPlayer</code>也可以用来播放<code>MPMediaItem</code>，这两个类的功能比较完善，推荐使用，在使用之前别忘记设置AudioSession；</p></li>
<li><p><code>MPMediaItem</code>可以得到对应的URL，这个URL可以用来做很多事情，例如用<code>AVAssetReader</code>和<code>AVAssetExportSession</code>可以导出其中的数据；</p></li>
</ul>


<hr />

<h1>下篇预告</h1>

<p>下一篇会讲一些关于NowPlayingCenter和RemoteControl的内容（就是在锁屏界面和ControlCenter中显示的歌曲信息以及上面的那些播放控制按钮）。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40008765">iPod Library Access Programming Guide</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/AboutiPodLibraryAccess/AboutiPodLibraryAccess.html#//apple_ref/doc/uid/TP40008765-CH103-SW9">About iPod Library Access</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingtheMediaItemPicker/UsingtheMediaItemPicker.html#//apple_ref/doc/uid/TP40008765-CH104-SW1">Using the Media Item Picker</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingTheiPodLibrary/UsingTheiPodLibrary.html#//apple_ref/doc/uid/TP40008765-CH101-SW1">Using the iPod Library</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingMediaPlayback/UsingMediaPlayback.html#//apple_ref/doc/uid/TP40008765-CH100-SW1">Using Media Playback</a></p>

<p><a href="http://www.subfurther.com/blog/2010/07/19/from-iphone-media-library-to-pcm-samples-in-dozens-of-confounding-potentially-lossy-steps/">From iPhone Media Library to PCM Samples in Dozens of Confounding, Potentially Lossy Steps</a></p>

<p><a href="http://www.subfurther.com/blog/2010/12/13/from-ipod-library-to-pcm-samples-in-far-fewer-steps-than-were-previously-necessary/">From iPod Library to PCM Samples in Far Fewer Steps Than Were Previously Necessary</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (六)：简单的音频播放器实现]]></title>
    <link href="http://msching.github.io/blog/2014/08/09/audio-in-ios-6/"/>
    <updated>2014-08-09T15:55:22+08:00</updated>
    <id>http://msching.github.io/blog/2014/08/09/audio-in-ios-6</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>在前几篇中我分别讲到了<code>AudioSession</code>、<code>AudioFileStream</code>、<code>AudioFile</code>、<code>AudioQueue</code>，这些类的功能已经涵盖了<a href="/blog/2014/07/07/audio-in-ios/">第一篇</a>中所提到的音频播放所需要的步骤：</p>

<ol>
<li>读取MP3文件   <code>NSFileHandle</code></li>
<li>解析采样率、码率、时长等信息，分离MP3中的音频帧  <code>AudioFileStream</code>/<code>AudioFile</code></li>
<li>对分离出来的音频帧解码得到PCM数据  <code>AudioQueue</code></li>
<li><del>对PCM数据进行音效处理（均衡器、混响器等，非必须）</del>  <code>省略</code></li>
<li>把PCM数据解码成音频信号  <code>AudioQueue</code></li>
<li>把音频信号交给硬件播放    <code>AudioQueue</code></li>
<li>重复1-6步直到播放完成</li>
</ol>


<p>下面我们就讲讲述如何用这些部件组成一个简单的<code>本地音乐播放器</code>，这里我会用到<strong>AudioSession</strong>、<strong>AudioFileStream</strong>、<strong>AudioFile</strong>、<strong>AudioQueue</strong>。</p>

<p><strong>注意：在阅读本篇请实现阅读并理解前面1-5篇的内容以及2-5篇最后给出的封装类，本篇中的播放器实现将基于前面几篇中给出的<a href="https://github.com/msching/MCAudioSession">MCAudioSession</a>、<a href="https://github.com/msching/MCAudioFileStream">MCAudioFileStream</a>、<a href="https://github.com/msching/MCAudioFile">MCAudioFile</a>和<a href="https://github.com/msching/MCSimpleAudioPlayer/blob/master/MCSimpleAudioPlayerDemo/MCSimpleAudioPlayer/MCAudioOutputQueue.h">MCAudioOutputQueue</a>进行实现。</strong></p>

<hr />

<h1>AudioFileStream vs AudioFile</h1>

<p>解释一下为什么我要同时使用<strong>AudioFileStream</strong>和<strong>AudioFile</strong>。</p>

<p>第一，<code>对于网络流播必须有AudioFileStream的支持</code>，这是因为我们在<a href="/blog/2014/07/19/audio-in-ios-4/">第四篇</a>中提到过<strong>AudioFile</strong>在Open时会要求使用者提供数据，如果提供的数据不足会直接跳过并且返回错误码，而数据不足的情况在网络流中很常见，故无法使用<strong>AudioFile</strong>单独进行网络流数据的解析；</p>

<p>第二，<code>对于本地音乐播放选用AudioFile更为合适</code>，原因如下：</p>

<ol>
<li><strong>AudioFileStream</strong>的主要是用在流播放中虽然不限于网络流和本地流，但流数据是按顺序提供的所以<strong>AudioFileStream</strong>也是顺序解析的，被解析的音频文件还是需要符合流播放的特性，对于不符合的本地文件<strong>AudioFileStream</strong>会在Parse时返回<code>NotOptimized</code>错误；</li>
<li><strong>AudioFile</strong>的解析过程并不是顺序的，它会在解析时通过回调向使用者索要某个位置的数据，即使数据在文件末尾也不要紧，所以<strong>AudioFile</strong>适用于所有类型的音频文件；</li>
</ol>


<p>基于以上两点我们可以得出这样一个结论：<code>一款完整功能的播放器应当同时使用AudioFileStream和AudioFile</code>，用<strong>AudioFileStream</strong>来应对可以进行流播放的音频数据，以达到边播放边缓冲的最佳体验，用<strong>AudioFile</strong>来处理无法流播放的音频数据，让用户在下载完成之后仍然能够进行播放。</p>

<p>本来这个Demo应该做成基于网络流的音频播放，但由于最近比较忙一直过着公司和床两点一线的生活，来不及写网络流和文件缓存的模块，所以就用本地文件代替了，所以最终在Demo会先尝试用<strong>AudioFileStream</strong>解析数据，如果失败再尝试使用<strong>AudioFile</strong>以达到模拟网络流播放的效果。</p>

<hr />

<h1>准备工作</h1>

<p>第一件事当然是要创建一个新工程，这里我选择了的模板是SingleView，工程名我把它命名为<code>MCSimpleAudioPlayerDemo</code>：</p>

<p><img src="/images/iOS-audio/createproject.jpg" alt="" /></p>

<p>创建完工程之后去到Target属性的<code>Capabilities</code>选项卡设置<code>Background Modes</code>，把<code>Audio and Airplay</code>勾选，这样我们的App就可以在进入后台之后继续播放音乐了：</p>

<p><img src="/images/iOS-audio/setBackgroundPlayback.jpg" alt="" /></p>

<p>接下来我们需要搭建一个简单的UI，在storyboard上创建两个UIButton和一个UISlider，Button用来做播放器的播放、暂停、停止等功能控制，Slider用来显示播放进度和seek。把这些UI组件和ViewController的属性/方法关联上之后简单的UI也就完成了。</p>

<p><img src="/images/iOS-audio/simpleUI.jpg" alt="" /></p>

<hr />

<h1>接口定义</h1>

<p>下面来创建播放器类<code>MCSimpleAudioPlayer</code>，首先是初始化方法（感谢<a href="http://weibo.com/onevcat?topnav=1&amp;wvr=5&amp;topsug=1">@喵神</a>的<a href="https://github.com/onevcat/VVDocumenter-Xcode">VVDocumenter</a>）：</p>

<p><code>objc
/**
 *  初始化方法
 *
 *  @param filePath 文件绝对路径
 *  @param fileType 文件类型，作为后续创建AudioFileStream和AudioQueue的Hint使用
 *
 *  @return player对象
 */
- (instancetype)initWithFilePath:(NSString *)filePath fileType:(AudioFileTypeID)fileType;
</code>
另外播放器作为一个典型的状态机，各种状态也是必不可少的，这里我只简单的定义了四种状态：</p>

<p>```objc
typedef NS_ENUM(NSUInteger, MCSAPStatus)
{</p>

<pre><code>MCSAPStatusStopped = 0,
MCSAPStatusPlaying = 1,
MCSAPStatusWaiting = 2,
MCSAPStatusPaused = 3,
</code></pre>

<p>};
```</p>

<p>再加上一些必不可少的属性和方法组成了<code>MCSimpleAudioPlayer.h</code></p>

<p>```objc
@interface MCSimpleAudioPlayer : NSObject</p>

<p>@property (nonatomic,copy,readonly) NSString *filePath;
@property (nonatomic,assign,readonly) AudioFileTypeID fileType;</p>

<p>@property (nonatomic,readonly) MCSAPStatus status;
@property (nonatomic,readonly) BOOL isPlayingOrWaiting;
@property (nonatomic,assign,readonly) BOOL failed;</p>

<p>@property (nonatomic,assign) NSTimeInterval progress;
@property (nonatomic,readonly) NSTimeInterval duration;</p>

<ul>
<li><p>(instancetype)initWithFilePath:(NSString *)filePath fileType:(AudioFileTypeID)fileType;</p></li>
<li><p>(void)play;</p></li>
<li>(void)pause;</li>
<li>(void)stop;
@end
```</li>
</ul>


<hr />

<h1>初始化</h1>

<p>在init方法中创建一个NSFileHandle的实例以用来读取数据并交给AudioFileStream解析，另外也可以根据生成的实例是否是nil来判断是否能够读取文件，如果返回的是nil就说明文件不存在或者没有权限那么播放也就无从谈起了。</p>

<p><code>objc
_fileHandler = [NSFileHandle fileHandleForReadingAtPath:_filePath];
</code></p>

<p>通过NSFileManager获取文件大小</p>

<p><code>objc
_fileSize = [[[NSFileManager defaultManager] attributesOfItemAtPath:_filePath error:nil] fileSize];
</code>
初始化方法到这里就结束了，作为一个播放器我们自然不能在主线程进行播放，我们需要创建自己的播放线程。</p>

<p>创建一个成员变量<code>_started</code>来表示播放流程是否已经开始，在<code>-play</code>方法中如果<code>_started</code>为NO就创建线程<code>_thread</code>并以<code>-threadMain</code>方法作为main，否则说明线程已经创建并且在播放流程中：</p>

<p>```
&ndash; (void)play
{</p>

<pre><code>if (!_started)
{
    _started = YES;
    _thread = [[NSThread alloc] initWithTarget:self selector:@selector(threadMain) object:nil];
    [_thread start];
}
else
{
    //如果是Pause状态就resume
}
</code></pre>

<p>}
<code>``
接下来就可以在</code>-threadMain`进行音频播放相关的操作了。</p>

<hr />

<h1>创建AudioSession</h1>

<p>iOS音频播放的第一步，自然是要创建<code>AudioSession</code>，这里引入<a href="/blog/2014/07/08/audio-in-ios-2/">第二篇</a>末尾给出的AudioSession封装<a href="https://github.com/msching/MCAudioSession">MCAudioSession</a>，当然各位也可以使用<code>AVAudioSession</code>。</p>

<p>初始化的工作会在调用单例方法时进行，下一步是设置Category。</p>

<p>```objc
//初始化并且设置Category
[[MCAudioSession sharedInstance] setCategory:kAudioSessionCategory_MediaPlayback error:NULL];</p>

<p>```
成功之后启用AudioSession，还有别忘了监听Interrupt通知。</p>

<p>```objc
if ([[MCAudioSession sharedInstance] setCategory:kAudioSessionCategory_MediaPlayback error:NULL])
{</p>

<pre><code>//active audiosession
[[NSNotificationCenter defaultCenter] addObserver:self selector:@selector(interruptHandler:) name:MCAudioSessionInterruptionNotification object:nil];
if ([[MCAudioSession sharedInstance] setActive:YES error:NULL])
{
    //go on
}
</code></pre>

<p>}
```</p>

<hr />

<h1>读取、解析音频数据</h1>

<p>成功创建并启用AudioSession之后就可以进入播放流程了，播放是一个无限循环的过程，所以我们需要一个while循环，在文件没有被播放完成之前需要反复的读取、解析、播放。那么第一步是需要读取并解析数据。按照之前说的我们会先使用<code>AudioFileStream</code>，引入<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>末尾给出的AudioFileStream封装<a href="https://github.com/msching/MCAudioFileStream">MCAudioFileStream</a>。</p>

<p>创建AudioFileStream，<strong>MCAudioFileStream</strong>的init方法会完成这项工作，如果创建成功就设置delegate作为Parse数据的回调。</p>

<p>```objc
<em>audioFileStream = [[MCAudioFileStream alloc] initWithFileType:</em>fileType fileSize:_fileSize error:&amp;error];
if (!error)
{</p>

<pre><code>_audioFileStream.delegate = self;
</code></pre>

<p>}
```</p>

<p>接下来要读取数据并且解析，用成员变量<code>_offset</code>表示<code>_fileHandler</code>已经读取文件位置，其主要作用是来判断Eof。调用<strong>MCAudioFileStream</strong>的<code>-parseData:error:</code>方法来对数据进行解析。</p>

<p>```objc
NSData *data = [<em>fileHandler readDataOfLength:1000];
</em>offset += [data length];
if (<em>offset >= </em>fileSize)
{</p>

<pre><code>isEof = YES;
</code></pre>

<p>}
[_audioFileStream parseData:data error:&amp;error];
if (error)
{</p>

<pre><code>//解析失败，换用AudioFile
</code></pre>

<p>}</p>

<p><code>``
解析完文件头之后**MCAudioFileStream**的</code>readyToProducePackets<code>属性会被置为YES，此后所有的Parse方法都回触发</code>-audioFileStream:audioDataParsed:<code>方法并传递</code>MCParsedAudioData`的数组来保存解析完成的数据。这样就需要一个buffer来存储这些解析完成的音频数据。</p>

<p>于是创建了<code>MCAudioBuffer</code>类来管理所有解析完成的数据：</p>

<p>```objc
@interface MCAudioBuffer : NSObject</p>

<ul>
<li><p>(instancetype)buffer;</p></li>
<li><p>(void)enqueueData:(MCParsedAudioData *)data;</p></li>
<li><p>(void)enqueueFromDataArray:(NSArray *)dataArray;</p></li>
<li><p>(BOOL)hasData;</p></li>
<li><p>(UInt32)bufferedSize;</p></li>
<li><p>(NSData <em>)dequeueDataWithSize:(UInt32)requestSize
                    packetCount:(UInt32 </em>)packetCount
                   descriptions:(AudioStreamPacketDescription **)descriptions;</p></li>
<li><p>(void)clean;
@end</p></li>
</ul>


<p>```</p>

<p>创建一个<strong>MCAudioBuffer</strong>的实例<code>_buffer</code>，解析完成的数据都会通过<code>enqueue</code>方法存储到<strong>_buffer</strong>中，在需要的使用可以通过<code>dequeue</code>取出来使用。</p>

<p>```objc
_buffer = [MCAudioBuffer buffer]; //初始化方法中创建</p>

<p>//AudioFileStream解析完成的数据都被存储到了_buffer中
&ndash; (void)audioFileStream:(MCAudioFileStream <em>)audioFileStream audioDataParsed:(NSArray </em>)audioData
{</p>

<pre><code>[_buffer enqueueFromDataArray:audioData];
</code></pre>

<p>}
```</p>

<p>如果遇到<strong>AudioFileStream</strong>解析失败的话，转而使用<strong>AudioFile</strong>，引入<a href="/blog/2014/07/19/audio-in-ios-4/">第四篇</a>末尾给出的AudioFile封装<a href="https://github.com/msching/MCAudioFile">MCAudioFile</a>（之前没有给出，最近补上的）。</p>

<p>```objc
_audioFileStream parseData:data error:&amp;error];
if (error)
{</p>

<pre><code>//解析失败，换用AudioFile
_usingAudioFile = YES;
continue;
</code></pre>

<p>}
```</p>

<p>```objc
if (_usingAudioFile)
{</p>

<pre><code>if (!_audioFile)
{
    _audioFile = [[MCAudioFile alloc] initWithFilePath:_filePath fileType:_fileType];
}
if ([_buffer bufferedSize] &lt; _bufferSize || !_audioQueue)
{    
    //AudioFile解析完成的数据都被存储到了_buffer中
    NSArray *parsedData = [_audioFile parseData:&amp;isEof];
    [_buffer enqueueFromDataArray:parsedData];
}
</code></pre>

<p>}</p>

<p>```</p>

<p>使用<strong>AudioFile</strong>时同样需要<strong>NSFileHandle</strong>来读取文件数据，但由于其回获取数据的特性我把FileHandle的相关操作都封装进去了，所以使用<strong>MCAudioFile</strong>解析数据时直接调用Parse方法即可。</p>

<hr />

<h1>播放</h1>

<p>有了解析完成的数据，接下来就该AudioQueue出场了，引入<a href="/blog/2014/08/02/audio-in-ios-5/">第五篇</a>末尾提到的AudioQueue的封装<a href="https://github.com/msching/MCSimpleAudioPlayer/blob/master/MCSimpleAudioPlayerDemo/MCSimpleAudioPlayer/MCAudioOutputQueue.h">MCAudioOutputQueue</a>。</p>

<p>首先创建AudioQueue，由于AudioQueue需要实现创建重用buffer所以需要事先确定bufferSize，这里我设置的bufferSize是近似0.1秒的数据量，计算bufferSize需要用到的duration和audioDataByteCount可以从<strong>MCAudioFileStream</strong>或者<strong>MCAudioFile</strong>中获取。有了bufferSize之后，加上数据格式format参数和magicCookie（部分音频格式需要）就可以生成AudioQueue了。</p>

<p>```objc
&ndash; (BOOL)createAudioQueue
{</p>

<pre><code>if (_audioQueue)
{
    return YES;
}

NSTimeInterval duration = _usingAudioFile ? _audioFile.duration : _audioFileStream.duration;
UInt64 audioDataByteCount = _usingAudioFile ? _audioFile.audioDataByteCount : _audioFileStream.audioDataByteCount;
_bufferSize = 0;
if (duration != 0)
{
    _bufferSize = (0.1 / duration) * audioDataByteCount;
}

if (_bufferSize &gt; 0)
{
    AudioStreamBasicDescription format = _usingAudioFile ? _audioFile.format : _audioFileStream.format;
    NSData *magicCookie = _usingAudioFile ? [_audioFile fetchMagicCookie] : [_audioFileStream fetchMagicCookie];
    _audioQueue = [[MCAudioOutputQueue alloc] initWithFormat:format bufferSize:_bufferSize macgicCookie:magicCookie];
    if (!_audioQueue.available)
    {
        _audioQueue = nil;
        return NO;
    }
}
return YES;
</code></pre>

<p>}
<code>
接下来从**_buffer**中读出解析完成的数据，交给AudioQueue播放。如果全部播放完毕了就调用一下`-flush`让AudioQueue把剩余数据播放完毕。这里需要注意的是**MCAudioOutputQueue**的`-playData`方法在调用时如果没有可以重用的buffer的话会阻塞当前线程直到`AudioQueue`回调方法送出可重用的buffer为止。
</code>
UInt32 packetCount;
AudioStreamPacketDescription <em>desces = NULL;
NSData </em>data = [<em>buffer dequeueDataWithSize:</em>bufferSize packetCount:&amp;packetCount descriptions:&amp;desces];
if (packetCount != 0)
{</p>

<pre><code>[_audioQueue playData:data packetCount:packetCount packetDescriptions:desces isEof:isEof];
free(desces);

if (![_buffer hasData] &amp;&amp; isEof)
{
    [_audioQueue flush];
    break;
}
</code></pre>

<p>}
```</p>

<hr />

<h1>暂停 &amp; 恢复</h1>

<p>暂停方法很简单，调用<strong>MCAudioOutputQueue</strong>的<code>-pause</code>方法就可以了，但要注意的是需要和<code>-playData:</code>同步调用，否则可能引起一些问题（比如触发了pause实际由于并发操作没有真正pause住）。</p>

<p>同步的方法可以采用加锁的方式，也可以通过标志位在<code>threadMain</code>中进行Pause，Demo中我使用了后者。</p>

<p>```objc
//pause方法
&ndash; (void)pause
{</p>

<pre><code>if (self.isPlayingOrWaiting)
{
    _pauseRequired = YES;
}
</code></pre>

<p>}</p>

<p>//threadMain中
&ndash; (void)threadMain
{</p>

<pre><code>...

//pause
if (_pauseRequired)
{
    [self setStatusInternal:MCSAPStatusPaused];
    [_audioQueue pause];
    [self _mutexWait];
    _pauseRequired = NO;
}

//play
...
</code></pre>

<p>}
```
在暂停后还要记得阻塞线程。</p>

<p>恢复只要调用<strong>AudioQueue</strong> start方法就可以了，同时记得signal让线程继续跑</p>

<p>```objc
&ndash; (void)_resume
{</p>

<pre><code>//AudioQueue的start方法被封装到了MCAudioOutputQueue的resume方法中
[_audioQueue resume];
[self _mutexSignal];
</code></pre>

<p>}
```</p>

<hr />

<h1>播放进度 &amp; Seek</h1>

<p>对于播放进度我在<a href="/blog/2014/08/02/audio-in-ios-5/">第五篇</a>讲<strong>AudioQueue</strong>时已经提到过了，使用<code>AudioQueueGetCurrentTime</code>方法可以获取<code>实际播放的时间</code>如果Seek之后需要根据计算timingOffset，然后根据timeOffset来计算最终的播放进度：</p>

<p>```objc
&ndash; (NSTimeInterval)progress
{</p>

<pre><code>return _timingOffset + _audioQueue.playedTime;
</code></pre>

<p>}
```
timingOffset的计算在Seek进行，Seek操作和暂停操作一样需要和其他<strong>AudioQueue</strong>的操作同步进行，否则可能造成一些并发问题。</p>

<p>```objc
//seek方法
&ndash; (void)setProgress:(NSTimeInterval)progress
{</p>

<pre><code>_seekRequired = YES;
_seekTime = progress;
</code></pre>

<p>}
```
在seek时为了防止播放进度跳动，修改一下获取播放进度的方法：</p>

<p>```objc
&ndash; (NSTimeInterval)progress
{</p>

<pre><code>if (_seekRequired)
{
    return _seekTime;
}
return _timingOffset + _audioQueue.playedTime;
</code></pre>

<p>}
<code>``
下面是</code>threadMain`中的Seek操作</p>

<p>```objc
if (_seekRequired)
{</p>

<pre><code>[self setStatusInternal:MCSAPStatusWaiting];

_timingOffset = _seekTime - _audioQueue.playedTime;
[_buffer clean];
if (_usingAudioFile)
{
    [_audioFile seekToTime:_seekTime];
}
else
{
    _offset = [_audioFileStream seekToTime:&amp;_seekTime];
    [_fileHandler seekToFileOffset:_offset];
}
_seekRequired = NO;
[_audioQueue reset];
</code></pre>

<p>}
```
Seek时需要做如下事情：</p>

<ol>
<li>计算timingOffset</li>
<li>清除之前残余在<code>_buffer</code>中的数据</li>
<li>挪动NSFileHandle的游标</li>
<li>清除<strong>AudioQueue</strong>中已经Enqueue的数据</li>
<li>如果有用到音效器的还需要清除音效器里的残余数据</li>
</ol>


<hr />

<h1>打断</h1>

<p>在接到Interrupt通知时需要处理打断，下面是打断的处理方法：</p>

<p>```objc
&ndash; (void)interruptHandler:(NSNotification *)notification
{</p>

<pre><code>UInt32 interruptionState = [notification.userInfo[MCAudioSessionInterruptionStateKey] unsignedIntValue];

if (interruptionState == kAudioSessionBeginInterruption)
{
    _pausedByInterrupt = YES;
    [_audioQueue pause];
    [self setStatusInternal:MCSAPStatusPaused];

}
else if (interruptionState == kAudioSessionEndInterruption)
{
    AudioSessionInterruptionType interruptionType = [notification.userInfo[MCAudioSessionInterruptionTypeKey] unsignedIntValue];
    if (interruptionType == kAudioSessionInterruptionType_ShouldResume)
    {
        if (self.status == MCSAPStatusPaused &amp;&amp; _pausedByInterrupt)
        {
            if ([[MCAudioSession sharedInstance] setActive:YES error:NULL])
            {
                [self play];
            }
        }
    }
}
</code></pre>

<p>}
```</p>

<p>这里需要注意，打断操作我放在了主线程进行而并非放到新开的线程中进行，原因如下：</p>

<ul>
<li><p>一旦打断开始<strong>AudioSession</strong>被抢占后音频立即被打断，此时<strong>AudioQueue</strong>的所有操作会暂停，这就意味着不会有任何数据消耗回调产生；</p></li>
<li><p>我这个Demo的线程模型中在向<strong>AudioQueue</strong> Enqueue了足够多的数据之后会阻塞当前线程等待数据消耗的回调才会signal让线程继续跑；</p></li>
</ul>


<p>于是就得到了这样的结论：一旦打断开始我创建的线程就会被阻塞，所以我需要在主线程来处理暂停和恢复播放。</p>

<hr />

<h1>停止 &amp; 清理</h1>

<p>停止操作也和其他操作一样会放到<code>threadMain</code>中执行</p>

<p>```objc
&ndash; (void)stop
{</p>

<pre><code>_stopRequired = YES;
[self _mutexSignal];
</code></pre>

<p>}</p>

<p>//treadMain中
if (_stopRequired)
{</p>

<pre><code>_stopRequired = NO;
_started = NO;
[_audioQueue stop:YES];
break;
</code></pre>

<p>}
```</p>

<p>在播放被停止或者出错时会进入到清理流程，这里需要做一大堆操作，清理各种数据，关闭AudioSession，清除各种标记等等。</p>

<p>```objc
&ndash; (void)cleanup
{</p>

<pre><code>//reset file
_offset = 0;
[_fileHandler seekToFileOffset:0];

//deactive audiosession
[[MCAudioSession sharedInstance] setActive:NO error:NULL];
[[NSNotificationCenter defaultCenter] removeObserver:self name:MCAudioSessionInterruptionNotification object:nil];

//clean buffer
[_buffer clean];

_usingAudioFile = NO;
//close audioFileStream
[_audioFileStream close];

//close audiofile
[_audioFile close];

//stop audioQueue
[_audioQueue stop:YES];

//destory mutex &amp; cond
[self _mutexDestory];

_started = NO;
_timingOffset = 0;
_seekTime = 0;
_seekRequired = NO;
_pauseRequired = NO;
_stopRequired = NO;

//reset status
[self setStatusInternal:MCSAPStatusStopped];
</code></pre>

<p>}</p>

<h2>```</h2>

<h1>连接播放器UI</h1>

<p>播放器代码完成后就需要和UI连起来让播放器跑起来了。</p>

<p>在viewDidLoad时创建一个播放器：</p>

<p>```objc</p>

<ul>
<li><p>(void)viewDidLoad
{
  [super viewDidLoad];</p>

<p>  if (!<em>player)
  {
      NSString *path = [[NSBundle mainBundle] pathForResource:@&ldquo;MP3Sample&rdquo; ofType:@&ldquo;mp3&rdquo;];
      </em>player = [[MCSimpleAudioPlayer alloc] initWithFilePath:path fileType:kAudioFileMP3Type];</p>

<pre><code>  [_player addObserver:self forKeyPath:@"status" options:NSKeyValueObservingOptionNew context:nil];
</code></pre>

<p>  }
  [_player play];
}
```
对播放器的status属性KVO用来操作播放和暂停按钮的状态以及播放进度timer的开启和暂停：</p></li>
</ul>


<p>```objc
&ndash; (void)observeValueForKeyPath:(NSString <em>)keyPath ofObject:(id)object change:(NSDictionary </em>)change context:(void *)context
{</p>

<pre><code>if (object == _player)
{
    if ([keyPath isEqualToString:@"status"])
    {
        [self performSelectorOnMainThread:@selector(handleStatusChanged) withObject:nil waitUntilDone:NO];
    }
}
</code></pre>

<p>}</p>

<ul>
<li><p>(void)handleStatusChanged
{
  if (_player.isPlayingOrWaiting)
  {
      [self.playOrPauseButton setTitle:@&ldquo;Pause&rdquo; forState:UIControlStateNormal];
      [self startTimer];</p>

<p>  }
  else
  {
      [self.playOrPauseButton setTitle:@&ldquo;Play&rdquo; forState:UIControlStateNormal];
      [self stopTimer];
      [self progressMove:nil];
  }
}</p></li>
</ul>


<p>```</p>

<p>播放进度交给timer来刷新：</p>

<p>```
&ndash; (void)startTimer
{</p>

<pre><code>if (!_timer)
{
    _timer = [NSTimer scheduledTimerWithTimeInterval:1 target:self selector:@selector(progressMove:) userInfo:nil repeats:YES];
    [_timer fire];
}
</code></pre>

<p>}</p>

<ul>
<li><p>(void)stopTimer
{
  if (<em>timer)
  {
      [</em>timer invalidate];
      _timer = nil;
  }
}</p></li>
<li><p>(void)progressMove:(id)sender
{
  //在seek时不要刷新slider的thumb位置
  if (!self.progressSlider.tracking)
  {
      if (<em>player.duration != 0)
      {
          self.progressSlider.value = </em>player.progress / _player.duration;
      }
      else
      {
          self.progressSlider.value = 0;
      }
  }
}
```</p></li>
</ul>


<p>监听slider的两个TouchUp时间来进行seek操作：</p>

<p>```objc
&ndash; (IBAction)seek:(id)sender
{</p>

<pre><code>_player.progress = _player.duration * self.progressSlider.value;
</code></pre>

<p>}
```</p>

<p>添加两个按钮的TouchUpInside事件进行播放控制：</p>

<p>```objc
&ndash; (IBAction)playOrPause:(id)sender
{</p>

<pre><code>if (_player.isPlayingOrWaiting)
{
    [_player pause];
}
else
{
    [_player play];
}
</code></pre>

<p>}</p>

<ul>
<li>(IBAction)stop:(id)sender
{
  [_player stop];
}</li>
</ul>


<p>```</p>

<hr />

<h1>进阶的内容</h1>

<p>关于简单播放器的构建就讲这么多，以下是一些音频播放相关的进阶内容，由于我自己也没有摸透它们所以暂时就不做详细介绍了以免误人子弟-_-，各位有兴趣可以研究一下，如果有疑问或者有新发现欢迎大家留言或者在<a href="http://weibo.com/msching">微博</a>上和我交流共同提高~</p>

<ol>
<li><code>AudioConverter</code>可以实现音频数据的转换，在播放流程中它可以充当解码器的角色，可以把压缩的音频数据解码成为PCM数据；</li>
<li><code>AudioUnit</code>作为比<code>AudioQueue</code>更底层的音频播放类库，Apple赋予了它更强大的功能，除了一般的播放功能之外它还能使用iPhone自带的多种均衡器对音效进行调节；</li>
<li><code>AUGraph</code>为<code>AudioUnit</code>提供音效处理功能（这个其实我一点也没接触过0_0）</li>
</ol>


<hr />

<h1>示例代码</h1>

<p>上面所讲述的内容对应的工程已经在github上了（<a href="https://github.com/msching/MCSimpleAudioPlayer">MCSimpleAudioPlayer</a>），有任何问题可以给我发issue~</p>

<hr />

<h1>下篇预告</h1>

<p>下一篇会介绍一下如何播放iOS系统<code>iPod Library</code>中的歌曲（俗称iPod音乐或者本地音乐）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (五)：AudioQueue]]></title>
    <link href="http://msching.github.io/blog/2014/08/02/audio-in-ios-5/"/>
    <updated>2014-08-02T14:21:13+08:00</updated>
    <id>http://msching.github.io/blog/2014/08/02/audio-in-ios-5</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>在<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>和<a href="/blog/2014/07/19/audio-in-ios-4/">第四篇</a>中介绍了如何用<code>AudioStreamFile</code>和<code>AudioFile</code>解析音频数据格式、分离音频帧。下一步终于可以使用分离出来的音频帧进行播放了，本片中将来讲一讲如何使用<code>AudioQueue</code>播放音频数据。</p>

<hr />

<h1>AudioQueue介绍</h1>

<p><code>AudioQueue</code>是<code>AudioToolBox.framework</code>中的一员，在<a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40005343">官方文档</a>中Apple这样描述<code>AudioQueue</code>的：</p>

<p><code>Audio Queue Services provides a straightforward, low overhead way to record and play audio in iOS and Mac OS X. It is the recommended technology to use for adding basic recording or playback features to your iOS or Mac OS X application.</code></p>

<p>在文档中Apple推荐开发者使用<code>AudioQueue</code>来实现app中的播放和录音功能。这里我们会针对播放功能进行介绍。</p>

<p>对于支持的数据格式，Apple这样说：</p>

<pre><code>Audio Queue Services lets you record and play audio in any of the following formats:

* Linear PCM.
* Any compressed format supported natively on the Apple platform you are developing for.
* Any other format for which a user has an installed codec.
</code></pre>

<p>它支持<code>PCM</code>数据、iOS/MacOSX平台支持的压缩格式（MP3、AAC等）、其他用户可以自行提供解码器的音频数据（对于这一条，我的理解就是把音频格式自行解码成PCM数据后再给AudioQueue播放  ）。</p>

<hr />

<h1>AudioQueue的工作模式</h1>

<p>在使用<code>AudioQueue</code>之前首先必须理解其工作模式，它之所以这么命名是因为在其内部有一套缓冲队列（Buffer Queue）的机制。在<code>AudioQueue</code>启动之后需要通过<code>AudioQueueAllocateBuffer</code>生成若干个<code>AudioQueueBufferRef</code>结构，这些Buffer将用来存储即将要播放的音频数据，并且这些Buffer是受生成他们的<code>AudioQueue</code>实例管理的，内存空间也已经被分配（按照Allocate方法的参数），当<code>AudioQueue</code>被Dispose时这些Buffer也会随之被销毁。</p>

<p>当有音频数据需要被播放时首先需要被memcpy到<code>AudioQueueBufferRef</code>的mAudioData中（mAudioData所指向的内存已经被分配，之前<code>AudioQueueAllocateBuffer</code>所做的工作），并给mAudioDataByteSize字段赋值传入的数据大小。完成之后需要调用<code>AudioQueueEnqueueBuffer</code>把存有音频数据的Buffer插入到<code>AudioQueue</code>内置的Buffer队列中。在Buffer队列中有buffer存在的情况下调用<code>AudioQueueStart</code>，此时<code>AudioQueue</code>就回按照Enqueue顺序逐个使用Buffer队列中的buffer进行播放，每当一个Buffer使用完毕之后就会从Buffer队列中被移除并且在使用者指定的RunLoop上触发一个回调来告诉使用者，某个<code>AudioQueueBufferRef</code>对象已经使用完成，你可以继续重用这个对象来存储后面的音频数据。如此循环往复音频数据就会被逐个播放直到结束。</p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AboutAudioQueues/AboutAudioQueues.html#//apple_ref/doc/uid/TP40005343-CH5-SW9">官方文档</a>给出了一副图来描述这一过程：</p>

<p>其中的callback按我的理解应该是指一个音频数据装填方法，该方法可以通过之前提到的数据使用后的回调来触发。</p>

<p><img src="/images/iOS-audio/audioqueuePlayback.jpg" alt="AudioQueue playback" /></p>

<p>根据Apple提供的<code>AudioQueue</code>工作原理结合自己理解，可以得到其工作流程大致如下：</p>

<ol>
<li>创建<code>AudioQueue</code>，创建一个自己的buffer数组BufferArray;</li>
<li>使用<code>AudioQueueAllocateBuffer</code>创建若干个<code>AudioQueueBufferRef</code>（一般2-3个即可），放入BufferArray；</li>
<li>有数据时从BufferArray取出一个buffer，memcpy数据后用<code>AudioQueueEnqueueBuffer</code>方法把buffer插入<code>AudioQueue</code>中；</li>
<li><code>AudioQueue</code>中存在Buffer后，调用<code>AudioQueueStart</code>播放。（具体等到填入多少buffer后再播放可以自己控制，只要能保证播放不间断即可）；</li>
<li><code>AudioQueue</code>播放音乐后消耗了某个buffer，在另一个线程回调并送出该buffer，把buffer放回BufferArray供下一次使用；</li>
<li>返回步骤3继续循环直到播放结束</li>
</ol>


<p>从以上步骤其实不难看出，<code>AudioQueue</code>播放的过程其实就是一个典型的<a href="http://zh.wikipedia.org/zh/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E9%97%AE%E9%A2%98">生产者消费者问题</a>。生产者是<code>AudioFileStream</code>或者<code>AudioFile</code>，它们生产处音频数据帧，放入到<code>AudioQueue</code>的buffer队列中，直到buffer填满后需要等待消费者消费；<code>AudioQueue</code>作为消费者，消费了buffer队列中的数据，并且在另一个线程回调通知数据已经被消费生产者可以继续生产。所以在实现<code>AudioQueue</code>播放音频的过程中必然会接触到一些多线程同步、信号量的使用、死锁的避免等等问题。</p>

<p>了解了工作流程之后再回头来看<code>AudioQueue</code>的方法，其中大部分方法都非常好理解，部分需要稍加解释。</p>

<hr />

<h1>创建AudioQueue</h1>

<p>使用下列方法来生成<code>AudioQueue</code>的实例
```objc
OSStatus AudioQueueNewOutput (const AudioStreamBasicDescription * inFormat,</p>

<pre><code>                            AudioQueueOutputCallback inCallbackProc,
                            void * inUserData,
                            CFRunLoopRef inCallbackRunLoop,
                            CFStringRef inCallbackRunLoopMode,
                            UInt32 inFlags,
                            AudioQueueRef * outAQ);
</code></pre>

<p>OSStatus AudioQueueNewOutputWithDispatchQueue(AudioQueueRef * outAQ,</p>

<pre><code>                                            const AudioStreamBasicDescription * inFormat,
                                            UInt32 inFlags,
                                            dispatch_queue_t inCallbackDispatchQueue,
                                            AudioQueueOutputCallbackBlock inCallbackBlock);
</code></pre>

<p>```
先来看第一个方法：</p>

<p>第一个参数表示需要播放的音频数据格式类型，是一个<code>AudioStreamBasicDescription</code>对象，是使用<code>AudioFileStream</code>或者<code>AudioFile</code>解析出来的数据格式信息；</p>

<p>第二个参数<code>AudioQueueOutputCallback</code>是某块Buffer<code>被使用之后</code>的回调；</p>

<p>第三个参数为上下文对象；</p>

<p>第四个参数inCallbackRunLoop为<code>AudioQueueOutputCallback</code>需要在的哪个RunLoop上被回调，如果传入NULL的话就会再<code>AudioQueue</code>的内部RunLoop中被回调，所以一般传NULL就可以了；</p>

<p>第五个参数inCallbackRunLoopMode为RunLoop模式，如果传入NULL就相当于<code>kCFRunLoopCommonModes</code>，也传NULL就可以了；</p>

<p>第六个参数inFlags是保留字段，目前没作用，传0；</p>

<p>第七个参数，返回生成的<code>AudioQueue</code>实例；</p>

<p>返回值用来判断是否成功创建（OSStatus == noErr）。</p>

<p>第二个方法就是把RunLoop替换成了一个dispatch queue，其余参数同相同。</p>

<hr />

<h1>Buffer相关的方法</h1>

<h3>1. 创建Buffer</h3>

<p>```objc
OSStatus AudioQueueAllocateBuffer(AudioQueueRef inAQ,</p>

<pre><code>                                UInt32 inBufferByteSize,
                                AudioQueueBufferRef * outBuffer);
</code></pre>

<p>OSStatus AudioQueueAllocateBufferWithPacketDescriptions(AudioQueueRef inAQ,</p>

<pre><code>                                                      UInt32 inBufferByteSize,
                                                      UInt32 inNumberPacketDescriptions,
                                                      AudioQueueBufferRef * outBuffer);
</code></pre>

<p>```</p>

<p>第一个方法传入<code>AudioQueue</code>实例和Buffer大小，传出的Buffer实例；</p>

<p>第二个方法可以指定生成的Buffer中PacketDescriptions的个数；</p>

<h3>2. 销毁Buffer</h3>

<p><code>objc
OSStatus AudioQueueFreeBuffer(AudioQueueRef inAQ,AudioQueueBufferRef inBuffer);
</code>
注意这个方法一般只在需要销毁特定某个buffer时才会被用到（因为dispose方法会自动销毁所有buffer），并且这个方法<code>只能在AudioQueue不在处理数据时</code>才能使用。所以这个方法一般不太能用到。</p>

<h3>3. 插入Buffer</h3>

<p>```objc
OSStatus AudioQueueEnqueueBuffer(AudioQueueRef inAQ,</p>

<pre><code>                               AudioQueueBufferRef inBuffer,
                               UInt32 inNumPacketDescs,
                               const AudioStreamPacketDescription * inPacketDescs);
</code></pre>

<p><code>``
Enqueue方法一共有两个，上面给出的是第一个方法，第二个方法</code>AudioQueueEnqueueBufferWithParameters`可以对Enqueue的buffer进行更多额外的操作，第二个方法我也没有细细研究，一般来说用第一个方法就能满足需求了，这里我也就只针对第一个方法进行说明：</p>

<p>这个Enqueue方法需要传入<code>AudioQueue</code>实例和需要Enqueue的Buffer，对于有inNumPacketDescs和inPacketDescs则需要根据需要选择传入，文档上说这两个参数主要是在播放VBR数据时使用，但之前我们提到过即便是CBR数据AudioFileStream或者AudioFile也会给出PacketDescription所以不能如此一概而论。简单的来说就是有就传PacketDescription没有就给NULL，不必管是不是VBR。</p>

<hr />

<h1>播放控制</h1>

<h3>1.开始播放</h3>

<p><code>objc
OSStatus AudioQueueStart(AudioQueueRef inAQ,const AudioTimeStamp * inStartTime);
</code></p>

<p>第二个参数可以用来控制播放开始的时间，一般情况下直接开始播放传入NULL即可。</p>

<h3>2.解码数据</h3>

<p>```objc
OSStatus AudioQueuePrime(AudioQueueRef inAQ,</p>

<pre><code>                        UInt32 inNumberOfFramesToPrepare,
                        UInt32 * outNumberOfFramesPrepared);                                    
</code></pre>

<p>```</p>

<p>这个方法并不常用，因为直接调用<code>AudioQueueStart</code>会自动开始解码（如果需要的话）。参数的作用是用来指定需要解码帧数和实际完成解码的帧数；</p>

<h3>3.暂停播放</h3>

<p><code>objc
OSStatus AudioQueuePause(AudioQueueRef inAQ);
</code></p>

<p>需要注意的是这个方法一旦调用后播放就会立即暂停，这就意味着<code>AudioQueueOutputCallback</code>回调也会暂停，这时需要特别关注线程的调度以防止线程陷入无限等待。</p>

<h3>4.停止播放</h3>

<p><code>objc
OSStatus AudioQueueStop(AudioQueueRef inAQ, Boolean inImmediate);
</code></p>

<p>第二个参数如果传入true的话会立即停止播放（同步），如果传入false的话<code>AudioQueue</code>会播放完已经Enqueue的所有buffer后再停止（异步）。使用时注意根据需要传入适合的参数。</p>

<h3>5.Flush</h3>

<p><code>objc
OSStatus
AudioQueueFlush(AudioQueueRef inAQ);
</code></p>

<p>调用后会播放完Enqueu的所有buffer后重置解码器状态，以防止当前的解码器状态影响到下一段音频的解码（比如切换播放的歌曲时）。如果和<code>AudioQueueStop(AQ,false)</code>一起使用并不会起效，因为Stop方法的false参数也会做同样的事情。</p>

<h3>6.重置</h3>

<p><code>objc
OSStatus AudioQueueReset(AudioQueueRef inAQ);
</code>
重置<code>AudioQueue</code>会清除所有已经Enqueue的buffer，并触发<code>AudioQueueOutputCallback</code>,调用<code>AudioQueueStop</code>方法时同样会触发该方法。这个方法的直接调用一般在seek时使用，用来清除残留的buffer（seek时还有一种做法是先<code>AudioQueueStop</code>，等seek完成后重新start）。</p>

<h3>7.获取播放时间</h3>

<p>```objc
OSStatus AudioQueueGetCurrentTime(AudioQueueRef inAQ,</p>

<pre><code>                                AudioQueueTimelineRef inTimeline,
                                AudioTimeStamp * outTimeStamp,
                                Boolean * outTimelineDiscontinuity);
</code></pre>

<p>```</p>

<p>传入的参数中，第一、第四个参数是和<code>AudioQueueTimeline</code>相关的我们这里并没有用到，传入NULL。调用后的返回<code>AudioTimeStamp</code>，从这个timestap结构可以得出播放时间，计算方法如下：</p>

<p><code>objc
AudioTimeStamp time = ...; //AudioQueueGetCurrentTime方法获取
NSTimeInterval playedTime = time.mSampleTime / _format.mSampleRate;
</code></p>

<p>在使用这个时间获取方法时有两点必须注意：</p>

<p>1、 第一个需要注意的时这个播放时间是指<code>实际播放的时间</code>和一般理解上的播放进度是有区别的。举个例子，开始播放8秒后用户操作slider把播放进度seek到了第20秒之后又播放了3秒钟，此时通常意义上播放时间应该是23秒，即播放进度；而用<code>GetCurrentTime</code>方法中获得的时间为11秒，即实际播放时间。所以每次seek时都必须保存seek的timingOffset：</p>

<p>```objc
AudioTimeStamp time = &hellip;; //AudioQueueGetCurrentTime方法获取
NSTimeInterval playedTime = time.mSampleTime / _format.mSampleRate; //seek时的播放时间</p>

<p>NSTimeInterval seekTime = &hellip;; //需要seek到哪个时间
NSTimeInterval timingOffset = seekTime &ndash; playedTime;
```</p>

<p>seek后的播放进度需要根据timingOffset和playedTime计算：
<code>objc
NSTimeInterval progress = timingOffset + playedTime;
</code></p>

<p>2、 第二个需要注意的是<code>GetCurrentTime</code>方法有时候会失败，所以上次获取的播放时间最好保存起来，如果遇到调用失败，就返回上次保存的结果。</p>

<hr />

<h1>销毁AudioQueue</h1>

<p><code>objc
AudioQueueDispose(AudioQueueRef inAQ,  Boolean inImmediate);
</code></p>

<p>销毁的同时会清除其中所有的buffer，第二个参数的意义和用法与<code>AudioQueueStop</code>方法相同。</p>

<p>这个方法使用时需要注意当<code>AudioQueueStart</code>调用之后<code>AudioQueue</code>其实还没有真正开始，期间会有一个短暂的间隙。如果在<code>AudioQueueStart</code>调用后到<code>AudioQueue</code>真正开始运作前的这段时间内调用<code>AudioQueueDispose</code>方法的话会导致程序卡死。这个问题是我在使用<a href="https://github.com/mattgallagher/AudioStreamer">AudioStreamer</a>时发现的，在iOS 6必现（iOS 7我倒是没有测试过，当时发现问题时iOS 7还没发布），起因是由于AudioStreamer会在音频EOF时就进入Cleanup环节，Cleanup环节会flush所有数据然后调用Dispose，那么当音频文件中数据非常少时就有可能出现<code>AudioQueueStart</code>调用之时就已经EOF进入Cleanup，此时就会出现上述问题。</p>

<p>要规避这个问题第一种方法是做好线程的调度，保证Dispose方法调用一定是在每一个播放RunLoop之后（即至少是一个buffer被成功播放之后）。第二种方法是监听<code>kAudioQueueProperty_IsRunning</code>属性，这个属性在<code>AudioQueue</code>真正运作起来之后会变成1，停止后会变成0，所以需要保证Start方法调用后Dispose方法一定要在<code>IsRunning</code>为1时才能被调用。</p>

<hr />

<h1>属性和参数</h1>

<p>和其他的<code>AudioToolBox</code>类一样，<code>AudioToolBox</code>有很多参数和属性可以设置、获取、监听。以下是相关的方法，这里就不再一一赘述：</p>

<p>```objc
//参数相关方法
AudioQueueGetParameter
AudioQueueSetParameter</p>

<p>//属性相关方法
AudioQueueGetPropertySize
AudioQueueGetProperty
AudioQueueSetProperty</p>

<p>//监听属性变化相关方法
AudioQueueAddPropertyListener
AudioQueueRemovePropertyListener
```</p>

<p>属性和参数的列表：
```
//属性列表
enum { // typedef UInt32 AudioQueuePropertyID</p>

<pre><code>kAudioQueueProperty_IsRunning               = 'aqrn',       // value is UInt32

kAudioQueueDeviceProperty_SampleRate        = 'aqsr',       // value is Float64
kAudioQueueDeviceProperty_NumberChannels    = 'aqdc',       // value is UInt32
kAudioQueueProperty_CurrentDevice           = 'aqcd',       // value is CFStringRef

kAudioQueueProperty_MagicCookie             = 'aqmc',       // value is void*
kAudioQueueProperty_MaximumOutputPacketSize = 'xops',       // value is UInt32
kAudioQueueProperty_StreamDescription       = 'aqft',       // value is AudioStreamBasicDescription

kAudioQueueProperty_ChannelLayout           = 'aqcl',       // value is AudioChannelLayout
kAudioQueueProperty_EnableLevelMetering     = 'aqme',       // value is UInt32
kAudioQueueProperty_CurrentLevelMeter       = 'aqmv',       // value is array of AudioQueueLevelMeterState, 1 per channel
kAudioQueueProperty_CurrentLevelMeterDB     = 'aqmd',       // value is array of AudioQueueLevelMeterState, 1 per channel

kAudioQueueProperty_DecodeBufferSizeFrames  = 'dcbf',       // value is UInt32
kAudioQueueProperty_ConverterError          = 'qcve',       // value is UInt32

kAudioQueueProperty_EnableTimePitch         = 'q_tp',       // value is UInt32, 0/1
kAudioQueueProperty_TimePitchAlgorithm      = 'qtpa',       // value is UInt32. See values below.
kAudioQueueProperty_TimePitchBypass         = 'qtpb',       // value is UInt32, 1=bypassed
</code></pre>

<p>};</p>

<p>//参数列表
enum    // typedef UInt32 AudioQueueParameterID;
{</p>

<pre><code>kAudioQueueParam_Volume         = 1,
kAudioQueueParam_PlayRate       = 2,
kAudioQueueParam_Pitch          = 3,
kAudioQueueParam_VolumeRampTime = 4,
kAudioQueueParam_Pan            = 13
</code></pre>

<p>};
```</p>

<p>其中比较有价值的属性有：</p>

<ul>
<li><code>kAudioQueueProperty_IsRunning</code>监听它可以知道当前<code>AudioQueue</code>是否在运行，这个参数的作用在讲到<code>AudioQueueDispose</code>时已经提到过。</li>
<li><code>kAudioQueueProperty_MagicCookie</code>部分音频格式需要设置magicCookie，这个cookie可以从<code>AudioFileStream</code>和<code>AudioFile</code>中获取。</li>
</ul>


<p>比较有价值的参数有：</p>

<ul>
<li><code>kAudioQueueParam_Volume</code>，它可以用来调节<code>AudioQueue</code>的播放音量，注意这个音量是<code>AudioQueue</code>的内部播放音量和系统音量相互独立设置并且最后叠加生效。</li>
<li><code>kAudioQueueParam_VolumeRampTime</code>参数和<code>Volume</code>参数配合使用可以实现音频播放淡入淡出的效果；</li>
<li><code>kAudioQueueParam_PlayRate</code>参数可以调整播放速率；</li>
</ul>


<hr />

<h1>后记</h1>

<p>至此本片关于<code>AudioQueue</code>的话题接结束了。使用上面提到的方法已经可以满足大部分的播放需求，但<code>AudioQueue</code>的功能远不止如此，<code>AudioQueueTimeline</code>、<code>Offline Rendering</code>、<code>AudioQueueProcessingTap</code>等功能我目前也尚未涉及和研究，未来也许还会有更多新的功能加入，学无止境啊。</p>

<p>另外由于<code>AudioQueue</code>的相关内容无法单独做Demo进行展示，于是我提前把后一篇内容的<a href="https://github.com/msching/MCSimpleAudioPlayer">Demo</a>（一个简单的本地音频播放器）先在这里给出方便大家理解<code>AudioQueue</code>。如果觉得上面提到某一部分的很难以的话理解欢迎在下面留言或者在<a href="http://weibo.com/msching">微博</a>上和我交流，除此之外还可以阅读官方文档（我一直觉得官方文档是学习的最好途径）；</p>

<hr />

<h1>示例代码</h1>

<p><a href="https://github.com/mattgallagher/AudioStreamer">AudioStreamer</a>和<a href="https://github.com/muhku/FreeStreamer">FreeStreamer</a>都用到了AudioQueue。在上面提到的Demo中也有我自己做的封装<a href="https://github.com/msching/MCSimpleAudioPlayer/blob/master/MCSimpleAudioPlayerDemo/MCSimpleAudioPlayer/MCAudioOutputQueue.h">MCAudioOutputQueue</a>。</p>

<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述如何利用之前讲到的<code>AudioSession</code>、<code>AudioFileStream</code>和<code>AudioQueue</code>实现一个简单的本地文件播放器。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40005343">Audio Queue Services Programming Guide</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AboutAudioQueues/AboutAudioQueues.html#//apple_ref/doc/uid/TP40005343-CH5-SW9">About Audio Queues</a></p>

<p><a href="http://zh.wikipedia.org/zh/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E9%97%AE%E9%A2%98">生产者消费者问题</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (四)：AudioFile]]></title>
    <link href="http://msching.github.io/blog/2014/07/19/audio-in-ios-4/"/>
    <updated>2014-07-19T13:38:30+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/19/audio-in-ios-4</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>接着<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>的<code>AudioStreamFile</code>这一篇要来聊一下<code>AudioFile</code>。和<code>AudioStreamFile</code>一样<code>AudioFile</code>是<code>AudioToolBox</code> framework中的一员，它也能够完成<a href="/blog/2014/07/07/audio-in-ios/">第一篇</a>所述的第2步，读取音频格式信息和进行帧分离，但事实上它的功能远不止如此。</p>

<hr />

<h1>AudioFile介绍</h1>

<p>按照<a href="https://developer.apple.com/library/mac/documentation/musicaudio/reference/AudioFileConvertRef/Reference/reference.html#//apple_ref/c/func/AudioFileCreateWithURL">官方文档</a>的描述：</p>

<p><code>a C programming interface that enables you to read or write a wide variety of audio data to or from disk or a memory buffer.With Audio File Services you can:</code></p>

<ul>
<li>Create, initialize, open, and close audio files</li>
<li>Read and write audio files</li>
<li>Optimize audio files</li>
<li>Work with user data and global information</li>
</ul>


<p>这个类可以用来创建、初始化音频文件；读写音频数据；对音频文件进行优化；读取和写入音频格式信息等等，功能十分强大，可见它不但可以用来支持音频播放，甚至可以用来生成音频文件。当然，在本篇文章中只会涉及一些和音频播放相关的内容（打开音频文件、读取格式信息、读取音频数据，其实我也只对这些方法有一点了解，其余的功能没用过。。>_&lt;）.</p>

<hr />

<h1>AudioFile的打开“姿势”</h1>

<p><code>AudioFile</code>提供了两个打开文件的方法：</p>

<p>1、 <code>AudioFileOpenURL</code></p>

<p>```objc</p>

<p>enum {</p>

<pre><code>kAudioFileReadPermission      = 0x01,
kAudioFileWritePermission     = 0x02,
kAudioFileReadWritePermission = 0x03
</code></pre>

<p>};</p>

<p>extern OSStatus AudioFileOpenURL (CFURLRef inFileRef,</p>

<pre><code>                                SInt8 inPermissions,
                                AudioFileTypeID inFileTypeHint,
                                AudioFileID * outAudioFile);
</code></pre>

<p>```</p>

<p>从方法的定义上来看是用来读取本地文件的：</p>

<p>第一个参数，文件路径；</p>

<p>第二个参数，文件的允许使用方式，是读、写还是读写，如果打开文件后进行了允许使用方式以外的操作，就得到<code>kAudioFilePermissionsError</code>错误码（比如Open时声明是<code>kAudioFileReadPermission</code>但却调用了<code>AudioFileWriteBytes</code>）；</p>

<p>第三个参数，和<code>AudioFileStream</code>的open方法中一样是一个帮助<code>AudioFile</code>解析文件的类型提示，如果文件类型确定的话应当传入；</p>

<p>第四个参数，返回AudioFile实例对应的<code>AudioFileID</code>，这个ID需要保存起来作为后续一些方法的参数使用；</p>

<p>返回值用来判断是否成功打开文件（OSSStatus == noErr）。</p>

<hr />

<p>2、 <code>AudioFileOpenWithCallbacks</code></p>

<p>```objc
extern OSStatus AudioFileOpenWithCallbacks (void * inClientData,</p>

<pre><code>                                          AudioFile_ReadProc inReadFunc,
                                          AudioFile_WriteProc inWriteFunc,
                                          AudioFile_GetSizeProc inGetSizeFunc,
                                          AudioFile_SetSizeProc inSetSizeFunc,
                                          AudioFileTypeID inFileTypeHint,
                                          AudioFileID * outAudioFile);
</code></pre>

<p>```</p>

<p>看过第一个Open方法后，这个方法乍看上去让人有点迷茫，没有URL的参数如何告诉AudioFile该打开哪个文件？还是先来看一下参数的说明吧：</p>

<p>第一个参数，上下文信息，不再多做解释；</p>

<p>第二个参数，当<code>AudioFile</code>需要读音频数据时进行的回调（调用Open和Read方式后<code>同步</code>回调）；</p>

<p>第三个参数，当<code>AudioFile</code>需要写音频数据时进行的回调（写音频文件功能时使用，暂不讨论）；</p>

<p>第四个参数，当<code>AudioFile</code>需要用到文件的总大小时回调（调用Open和Read方式后<code>同步</code>回调）；</p>

<p>第五个参数，当<code>AudioFile</code>需要设置文件的大小时回调（写音频文件功能时使用，暂不讨论）；</p>

<p>第六、七个参数和返回值同<code>AudioFileOpenURL</code>方法；</p>

<p>这个方法的重点在于<code>AudioFile_ReadProc</code>这个回调。换一个角度理解，这个方法相比于第一个方法自由度更高，AudioFile需要的只是一个数据源，无论是磁盘上的文件、内存里的数据甚至是网络流只要能在<code>AudioFile</code>需要数据时（Open和Read时）通过<code>AudioFile_ReadProc</code>回调为AudioFile提供合适的数据就可以了，也就是说使用方法不仅仅可以读取本地文件也可以如<code>AudioFileStream</code>一样以流的形式读取数据。</p>

<hr />

<p>下面来看一下<code>AudioFile_GetSizeProc</code>和<code>AudioFile_ReadProc</code>这两个读取功能相关的回调
```objc
typedef SInt64 (*AudioFile_GetSizeProc)(void * inClientData);</p>

<p>typedef OSStatus (*AudioFile_ReadProc)(void * inClientData,</p>

<pre><code>                                     SInt64 inPosition,
                                     UInt32 requestCount,
                                     void * buffer, 
                                     UInt32 * actualCount);
</code></pre>

<p>```</p>

<p>首先是<code>AudioFile_GetSizeProc</code>回调，这个回调很好理解，返回文件总长度即可，总长度的获取途径自然是文件系统或者httpResponse等等。</p>

<p>接下来是<code>AudioFile_ReadProc</code>回调：</p>

<p>第一个参数，上下文对象，不再赘述；</p>

<p>第二个参数，需要读取第几个字节开始的数据；</p>

<p>第三个参数，需要读取的数据长度；</p>

<p>第四个参数，返回参数，是一个数据指针并且其空间已经被分配，我们需要做的是把数据memcpy到buffer中；</p>

<p>第五个参数，实际提供的数据长度，即memcpy到buffer中的数据长度；</p>

<p>返回值，如果没有任何异常产生就返回noErr，如果有异常可以根据异常类型选择需要的error常量返回（一般用不到其他返回值，返回noErr就足够了）；</p>

<p>这里需要解释一下这个回调方法的工作方式。<code>AudioFile</code>需要数据时会调用回调方法，需要数据的时间点有两个：</p>

<ol>
<li><p>Open方法调用时，由于<code>AudioFile</code>的Open方法调用过程中就会对音频格式信息进行解析，只有符合要求的音频格式才能被成功打开否则Open方法就会返回错误码（换句话说，Open方法一旦调用成功就相当于<code>AudioStreamFile</code>在Parse后返回<code>ReadyToProducePackets</code>一样，只要Open成功就可以开始读取音频数据，详见<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>），所以在Open方法调用的过程中就需要提供一部分音频数据来进行解析；</p></li>
<li><p>Read相关方法调用时，这个不需要多说很好理解；</p></li>
</ol>


<p>通过回调提供数据时需要注意inPosition和requestCount参数，这两个参数指明了本次回调需要提供的数据范围是从inPosition开始requestCount个字节的数据。这里又可以分为两种情况：</p>

<ol>
<li><p>有充足的数据：那么我们需要把这个范围内的数据拷贝到buffer中，并且给actualCount赋值requestCount，最后返回noError；</p></li>
<li><p>数据不足：没有充足数据的话就只能把手头有的数据拷贝到buffer中，需要注意的是这部分被拷贝的数据必须是从inPosition开始的<code>连续数据</code>，拷贝完成后给actualCount赋值实际拷贝进buffer中的数据长度后返回noErr，这个过程可以用下面的代码来表示：</p></li>
</ol>


<p>```objc
static OSStatus MyAudioFileReadCallBack(void *inClientData,</p>

<pre><code>                                    SInt64 inPosition,
                                    UInt32 requestCount,
                                    void *buffer,
                                    UInt32 *actualCount)
</code></pre>

<p>{</p>

<pre><code>__unsafe_unretained MyContext *context = (__bridge MyContext *)inClientData;

*actualCount = [context availableDataLengthAtOffset:inPosition maxLength:requestCount];
if (*actualCount &gt; 0)
{
    NSData *data = [context dataAtOffset:inPosition length:*actualCount];
    memcpy(buffer, [data bytes], [data length]);
}

return noErr;
</code></pre>

<p>}</p>

<p>```</p>

<p>说到这里又需要分两种情况：</p>

<p>2.1. Open方法调用时的回调数据不足：AudioFile的Open方法会根据文件格式类型分几步进行数据读取以解析确定是否是一个合法的文件格式，其中每一步的inPosition和requestCount都不一样，如果某一步不成功就会直接进行下一步，如果几部下来都失败了，那么Open方法就会失败。简单的说就是在调用Open之前首先需要保证音频文件的格式信息完整，这就意味着<code>AudioFile</code>并不能独立用于音频流的读取，在流播放时首先需要使用<code>AudioStreamFile</code>来得到<code>ReadyToProducePackets</code>标志位来保证信息完整；</p>

<p>2.2. Read方法调用时的回调数据不足：这种情况下inPosition和requestCount的数值与Read方法调用时传入的参数有关，数据不足对于Read方法本身没有影响，只要回调返回noErr，Read就成功，只是实际交给Read方法的调用方的数据会不足，那么就把这个问题的处理交给了Read的调用方；</p>

<hr />

<h1>读取音频格式信息</h1>

<p>成功打开音频文件后就可以读取其中的格式信息了，读取用到的方法如下：</p>

<p>```objc
extern OSStatus AudioFileGetPropertyInfo(AudioFileID inAudioFile,</p>

<pre><code>                                       AudioFilePropertyID inPropertyID,
                                       UInt32 * outDataSize,
                                       UInt32 * isWritable);
</code></pre>

<p>extern OSStatus AudioFileGetProperty(AudioFileID inAudioFile,</p>

<pre><code>                                   AudioFilePropertyID inPropertyID,
                                   UInt32 * ioDataSize,
                                   void * outPropertyData); 
</code></pre>

<p><code>``
</code>AudioFileGetPropertyInfo<code>方法用来获取某个属性对应的数据的大小（outDataSize）以及该属性是否可以被write（isWritable），而</code>AudioFileGetProperty<code>则用来获取属性对应的数据。对于一些大小可变的属性需要先使用</code>AudioFileGetPropertyInfo<code>获取数据大小才能取获取数据（例如formatList），而有些确定类型单个属性则不必先调用</code>AudioFileGetPropertyInfo<code>直接调用</code>AudioFileGetProperty`即可（比如BitRate），例子如下：</p>

<p>```objc
AudioFileID fileID; //Open方法返回的AudioFileID</p>

<p>//获取格式信息
UInt32 formatListSize = 0;
OSStatus status = AudioFileGetPropertyInfo(_fileID, kAudioFilePropertyFormatList, &amp;formatListSize, NULL);
if (status == noErr)
{</p>

<pre><code>AudioFormatListItem *formatList = (AudioFormatListItem *)malloc(formatListSize);
status = AudioFileGetProperty(fileID, kAudioFilePropertyFormatList, &amp;formatListSize, formatList);
if (status == noErr)
{
    for (int i = 0; i * sizeof(AudioFormatListItem) &lt; formatListSize; i += sizeof(AudioFormatListItem))
    {
        AudioStreamBasicDescription pasbd = formatList[i].mASBD;
        //选择需要的格式。。                             
    }
}
free(formatList);
</code></pre>

<p>}</p>

<p>//获取码率
UInt32 bitRate;
UInt32 bitRateSize = sizeof(bitRate);
status = AudioFileGetProperty(fileID, kAudioFilePropertyBitRate, &amp;size, &amp;bitRate);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}
```
可以获取的属性有下面这些，大家可以参考文档来获取自己需要的信息（注意到这里有EstimatedDuration，可以得到Duration了）：</p>

<p>```
enum
{</p>

<pre><code>kAudioFilePropertyFileFormat                =   'ffmt',
kAudioFilePropertyDataFormat                =   'dfmt',
kAudioFilePropertyIsOptimized           =   'optm',
kAudioFilePropertyMagicCookieData       =   'mgic',
kAudioFilePropertyAudioDataByteCount        =   'bcnt',
kAudioFilePropertyAudioDataPacketCount  =   'pcnt',
kAudioFilePropertyMaximumPacketSize     =   'psze',
kAudioFilePropertyDataOffset                =   'doff',
kAudioFilePropertyChannelLayout         =   'cmap',
kAudioFilePropertyDeferSizeUpdates      =   'dszu',
kAudioFilePropertyMarkerList                =   'mkls',
kAudioFilePropertyRegionList                =   'rgls',
kAudioFilePropertyChunkIDs              =   'chid',
kAudioFilePropertyInfoDictionary            =   'info',
kAudioFilePropertyPacketTableInfo       =   'pnfo',
kAudioFilePropertyFormatList                =   'flst',
kAudioFilePropertyPacketSizeUpperBound      =   'pkub',
kAudioFilePropertyReserveDuration       =   'rsrv',
kAudioFilePropertyEstimatedDuration     =   'edur',
kAudioFilePropertyBitRate               =   'brat',
kAudioFilePropertyID3Tag                    =   'id3t',
kAudioFilePropertySourceBitDepth            =   'sbtd',
kAudioFilePropertyAlbumArtwork          =   'aart',
</code></pre>

<p>  kAudioFilePropertyAudioTrackCount         =    &lsquo;atct&rsquo;,</p>

<pre><code>kAudioFilePropertyUseAudioTrack         =   'uatk'
</code></pre>

<p>};<br/>
```</p>

<hr />

<h1>读取音频数据</h1>

<p>读取音频数据的方法分为两类：</p>

<p>1、直接读取音频数据：</p>

<p>```objc
extern OSStatus AudioFileReadBytes (AudioFileID inAudioFile,</p>

<pre><code>                                  Boolean inUseCache,
                                  SInt64 inStartingByte,
                                  UInt32 * ioNumBytes,
                                  void * outBuffer);
</code></pre>

<p>```</p>

<p>第一个参数，FileID；</p>

<p>第二个参数，是否需要cache，一般来说传false；</p>

<p>第三个参数，从第几个byte开始读取数据</p>

<p>第四个参数，这个参数在调用时作为输入参数表示需要读取读取多少数据，调用完成后作为输出参数表示实际读取了多少数据（即Read回调中的requestCount和actualCount）；</p>

<p>第五个参数，buffer指针，需要事先分配好足够大的内存（ioNumBytes大，即Read回调中的buffer，所以Read回调中不需要再分配内存）；</p>

<p>返回值表示是否读取成功，EOF时会返回<code>kAudioFileEndOfFileError</code>；</p>

<p>使用这个方法得到的数据都是没有进行过帧分离的数据，如果想要用来播放或者解码还必须通过<code>AudioFileStream</code>进行帧分离；</p>

<p>2、按帧（Packet）读取音频数据：</p>

<p>```objc
extern OSStatus AudioFileReadPacketData (AudioFileID inAudioFile,</p>

<pre><code>                                       Boolean inUseCache,
                                       UInt32 * ioNumBytes,
                                       AudioStreamPacketDescription * outPacketDescriptions,
                                       SInt64 inStartingPacket,
                                       UInt32 * ioNumPackets,
                                       void * outBuffer);
</code></pre>

<p>extern OSStatus AudioFileReadPackets (AudioFileID inAudioFile,</p>

<pre><code>                                    Boolean inUseCache,
                                    UInt32 * outNumBytes,
                                    AudioStreamPacketDescription * outPacketDescriptions,
                                    SInt64 inStartingPacket, 
                                    UInt32 * ioNumPackets, 
                                    void * outBuffer);
</code></pre>

<p>```
按帧读取的方法有两个，这两个方法看上去差不多，就连参数也几乎相同，但使用场景和效率上却有所不同，<a href="https://developer.apple.com/library/mac/documentation/musicaudio/reference/AudioFileConvertRef/Reference/reference.html#//apple_ref/c/func/AudioFileCreateWithURL">官方文档</a>中如此描述这两个方法：</p>

<ul>
<li><code>AudioFileReadPacketData</code> is memory efficient when reading variable bit-rate (VBR) audio data;</li>
<li><code>AudioFileReadPacketData</code> is more efficient than <code>AudioFileReadPackets</code> when reading compressed file formats that do not have packet tables, such as MP3 or ADTS. This function is a good choice for reading either CBR (constant bit-rate) or VBR data if you do not need to read a fixed duration of audio.</li>
<li>Use <code>AudioFileReadPackets</code> only when you need to read a fixed duration of audio data, or when you are reading only uncompressed audio.</li>
</ul>


<p>只有当需要读取固定时长音频或者非压缩音频时才会用到<code>AudioFileReadPackets</code>，其余时候使用<code>AudioFileReadPacketData</code>会有更高的效率并且更省内存；</p>

<p>下面来看看这些参数：</p>

<p>第一、二个参数，同<code>AudioFileReadBytes</code>；</p>

<p>第三个参数，对于<code>AudioFileReadPacketData</code>来说ioNumBytes这个参数在输入输出时都要用到，在输入时表示outBuffer的size，输出时表示实际读取了多少size的数据。而对<code>AudioFileReadPackets</code>来说outNumBytes只在输出时使用，表示实际读取了多少size的数据；</p>

<p>第四个参数，帧信息数组指针，在输入前需要分配内存，大小必须足够存在ioNumPackets个帧信息（ioNumPackets * sizeof(AudioStreamPacketDescription)）；</p>

<p>第五个参数，从第几帧开始读取数据；</p>

<p>第六个参数，在输入时表示需要读取多少个帧，在输出时表示实际读取了多少帧；</p>

<p>第七个参数，outBuffer数据指针，在输入前就需要分配好空间，这个参数看上去两个方法一样但其实并非如此。对于<code>AudioFileReadPacketData</code>来说只要分配<code>近似帧大小 * 帧数</code>的内存空间即可，方法本身会针对给定的内存空间大小来决定最后输出多少个帧，如果空间不够会适当减少出的帧数；而对于<code>AudioFileReadPackets</code>来说则需要分配<code>最大帧大小(或帧大小上界) * 帧数</code>的内存空间才行（最大帧大小和帧大小上界的区别等下会说）；这也就是为何第三个参数一个是输入输出双向使用的，而另一个只是输出时使用的原因。就这点来说两个方法中前者在使用的过程中要比后者更省内存；</p>

<p>返回值，同<code>AudioFileReadBytes</code>；</p>

<p>这两个方法读取后的数据为帧分离后的数据，可以直接用来播放或者解码。</p>

<p>下面给出两个方法的使用代码（以MP3为例）：</p>

<p>```objc
AudioFileID fileID; //Open方法返回的AudioFileID
UInt32 ioNumPackets = &hellip;; //要读取多少个packet
SInt64 inStartingPacket = &hellip;; //从第几个Packet开始读取</p>

<p>UInt32 bitRate = &hellip;; //AudioFileGetProperty读取kAudioFilePropertyBitRate
UInt32 sampleRate = &hellip;; //AudioFileGetProperty读取kAudioFilePropertyDataFormat或kAudioFilePropertyFormatList
UInt32 byteCountPerPacket = 144 * bitRate / sampleRate; //MP3数据每个Packet的近似大小</p>

<p>UInt32 descSize = sizeof(AudioStreamPacketDescription) * ioNumPackets;
AudioStreamPacketDescription * outPacketDescriptions = (AudioStreamPacketDescription *)malloc(descSize);</p>

<p>UInt32 ioNumBytes = byteCountPerPacket * ioNumPackets;
void * outBuffer = (void *)malloc(ioNumBytes);</p>

<p>OSStatus status = AudioFileReadPacketData(fileID,</p>

<pre><code>                                        false, 
                                        &amp;ioNumBytes, 
                                        outPacketDescriptions, 
                                        inStartingPacket, 
                                        &amp;ioNumPackets, 
                                        outBuffer);
</code></pre>

<p>```</p>

<p>```objc
AudioFileID fileID; //Open方法返回的AudioFileID
UInt32 ioNumPackets = &hellip;; //要读取多少个packet
SInt64 inStartingPacket = &hellip;; //从第几个Packet开始读取</p>

<p>UInt32 maxByteCountPerPacket = &hellip;; //AudioFileGetProperty读取kAudioFilePropertyMaximumPacketSize，最大的packet大小
//也可以用：
//UInt32 byteCountUpperBoundPerPacket = &hellip;; //AudioFileGetProperty读取kAudioFilePropertyPacketSizeUpperBound，当前packet大小上界（未扫描全文件的情况下）</p>

<p>UInt32 descSize = sizeof(AudioStreamPacketDescription) * ioNumPackets;
AudioStreamPacketDescription * outPacketDescriptions = (AudioStreamPacketDescription *)malloc(descSize);</p>

<p>UInt32 outNumBytes = 0；
UInt32 ioNumBytes = maxByteCountPerPacket * ioNumPackets;
void * outBuffer = (void *)malloc(ioNumBytes);</p>

<p>OSStatus status = AudioFileReadPackets(fileID,</p>

<pre><code>                                   false,
                                   &amp;outNumBytes,
                                   outPacketDescriptions,
                                   inStartingPacket,
                                   &amp;ioNumPackets,
                                   outBuffer);
</code></pre>

<p>```</p>

<hr />

<h1>Seek</h1>

<p>seek的思路和之前讲<code>AudioFileStream</code>时讲到的是一样的，区别在于AudioFile没有方法来帮助修正seek的offset和seek的时间：</p>

<ul>
<li>使用<code>AudioFileReadBytes</code>时需要计算出approximateSeekOffset</li>
<li>使用<code>AudioFileReadPacketData</code>或者<code>AudioFileReadPackets</code>时需要计算出seekToPacket</li>
</ul>


<p>approximateSeekOffset和seekToPacket的计算方法参见<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>。</p>

<hr />

<h1>关闭AudioFile</h1>

<p><code>AudioFile</code>使用完毕后需要调用<code>AudioFileClose</code>进行关闭，没啥特别需要注意的。</p>

<p><code>objc
extern OSStatus AudioFileClose (AudioFileID inAudioFile);   
</code></p>

<hr />

<h1>小结</h1>

<p>本篇针对<code>AudioFile</code>的音频读取功能做了介绍，小结一下：</p>

<ul>
<li><p><code>AudioFile</code>有两个Open方法，需要针对自身的使用场景选择不同的方法；</p></li>
<li><p><code>AudioFileOpenURL</code>用来读取本地文件</p></li>
<li><p><code>AudioFileOpenWithCallbacks</code>的使用场景比前者要广泛，使用时需要注意<code>AudioFile_ReadProc</code>，这个回调方法在Open方法本身和Read方法被调用时会被<code>同步</code>调用</p></li>
<li><p>必须保证音频文件格式信息可读时才能使用<code>AudioFile</code>的Open方法，AudioFile并不能独立用于音频流的读取，需要配合<code>AudioStreamFile</code>使用才能读取流（需要用<code>AudioStreamFile</code>来判断文件格式信息可读之后再调用Open方法）；</p></li>
<li><p>使用<code>AudioFileGetProperty</code>读取格式信息时需要判断所读取的信息是否需要先调用<code>AudioFileGetPropertyInfo</code>获得数据大小后再进行读取；</p></li>
<li><p>读取音频数据应该根据使用的场景选择不同的音频读取方法，对于不同的读取方法seek时需要计算的变量也不相同；</p></li>
<li><p><code>AudioFile</code>使用完毕后需要调用<code>AudioFileClose</code>进行关闭；</p></li>
</ul>


<hr />

<h1>示例代码</h1>

<p><del>对于本地文件用AudioFile读取比较简单就不在这里提供demo了，</del></p>

<p><a href="https://github.com/msching/MCAudioFile">简单的AudioFile封装</a>。</p>

<p>对于流播放中的AudioFile使用推荐大家阅读豆瓣的开源播放器代码<a href="https://github.com/douban/DOUAudioStreamer">DOUAudioStreamer</a>。</p>

<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述如何使用<code>AudioQueue</code>。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/mac/documentation/musicaudio/reference/AudioFileConvertRef/Reference/reference.html#//apple_ref/c/func/AudioFileCreateWithURL">Audio File Services Reference</a></p>
]]></content>
  </entry>
  
</feed>
